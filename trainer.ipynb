{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Lec5-RL-Gymnasium.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.frame_stack import FrameStack\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 0\n",
    "max_frames = 1e7\n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = AtariPreprocessing(env)\n",
    "\n",
    "env = FrameStack(env, 4)\n",
    "def trigger(t):\n",
    "    return t % 100 == 0\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observation):\n",
    "    observation = tf.transpose(observation, perm=[0, 1, 2])\n",
    "    observation = tf.cast(observation, tf.float32) / 255.0\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width, channels = env.observation_space.shape\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "\n",
    "def create_q_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            # layers.InputLayer(shape=(3, height, width, channels)),\n",
    "            layers.Permute((2, 3, 1)),  # Rearrange dimensions\n",
    "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# observation, _ = env.reset(seed=42)\n",
    "# state = np.array(observation)\n",
    "# state_tensor = keras.ops.convert_to_tensor(state)\n",
    "# state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "# print(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ permute_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ permute_6 (\u001b[38;5;33mPermute\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "max_buffer_size = 100000\n",
    "action_history = deque(maxlen=max_buffer_size)\n",
    "state_history = deque(maxlen=max_buffer_size)\n",
    "state_next_history = deque(maxlen=max_buffer_size)\n",
    "done_history = deque(maxlen=max_buffer_size)\n",
    "rewards_history = deque(maxlen=max_buffer_size)\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 1000000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 6\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of last 100: 385.0, running reward: 151.25 at episode 20, frame count 10000, time: 222.19386291503906\n",
      "best score of last 100: 410.0, running reward: 147.68 at episode 41, frame count 20000, time: 445.94836807250977\n",
      "best score of last 100: 500.0, running reward: 160.88 at episode 57, frame count 30000, time: 670.8054659366608\n",
      "best score of last 100: 535.0, running reward: 155.72 at episode 76, frame count 40000, time: 896.20130610466\n",
      "best score of last 100: 610.0, running reward: 160.63 at episode 95, frame count 50000, time: 1123.9240419864655\n",
      "best score of last 100: 610.0, running reward: 159.40 at episode 114, frame count 60000, time: 1358.9138169288635\n",
      "best score of last 100: 610.0, running reward: 162.10 at episode 133, frame count 70000, time: 1595.4292950630188\n",
      "best score of last 100: 610.0, running reward: 151.60 at episode 154, frame count 80000, time: 1832.0393500328064\n",
      "best score of last 100: 610.0, running reward: 149.55 at episode 174, frame count 90000, time: 2070.8949539661407\n",
      "best score of last 100: 395.0, running reward: 148.00 at episode 192, frame count 100000, time: 2311.6988401412964\n",
      "best score of last 100: 395.0, running reward: 154.10 at episode 211, frame count 110000, time: 2555.082041978836\n",
      "best score of last 100: 410.0, running reward: 145.40 at episode 232, frame count 120000, time: 2798.904606103897\n",
      "best score of last 100: 410.0, running reward: 151.20 at episode 252, frame count 130000, time: 3044.205421924591\n",
      "best score of last 100: 410.0, running reward: 146.55 at episode 273, frame count 140000, time: 3293.515326023102\n",
      "best score of last 100: 410.0, running reward: 139.60 at episode 293, frame count 150000, time: 3540.6685061454773\n",
      "best score of last 100: 440.0, running reward: 138.55 at episode 313, frame count 160000, time: 3788.5640881061554\n",
      "best score of last 100: 440.0, running reward: 144.60 at episode 332, frame count 170000, time: 4043.4826629161835\n",
      "best score of last 100: 490.0, running reward: 147.60 at episode 352, frame count 180000, time: 4303.467427968979\n",
      "best score of last 100: 490.0, running reward: 153.20 at episode 372, frame count 190000, time: 4566.510314941406\n",
      "best score of last 100: 515.0, running reward: 158.55 at episode 391, frame count 200000, time: 4829.8981590271\n",
      "best score of last 100: 515.0, running reward: 163.65 at episode 412, frame count 210000, time: 5084.528058052063\n",
      "best score of last 100: 515.0, running reward: 165.80 at episode 432, frame count 220000, time: 5342.162029027939\n",
      "best score of last 100: 515.0, running reward: 161.00 at episode 453, frame count 230000, time: 5606.803276062012\n",
      "best score of last 100: 515.0, running reward: 165.35 at episode 471, frame count 240000, time: 5866.733612060547\n",
      "best score of last 100: 515.0, running reward: 168.40 at episode 489, frame count 250000, time: 6136.545865058899\n",
      "best score of last 100: 515.0, running reward: 172.05 at episode 508, frame count 260000, time: 6398.730111122131\n",
      "best score of last 100: 515.0, running reward: 168.65 at episode 528, frame count 270000, time: 6660.291975975037\n",
      "best score of last 100: 515.0, running reward: 172.55 at episode 547, frame count 280000, time: 6934.498518943787\n",
      "best score of last 100: 515.0, running reward: 170.70 at episode 566, frame count 290000, time: 7201.102324008942\n",
      "best score of last 100: 610.0, running reward: 174.40 at episode 584, frame count 300000, time: 7463.286687135696\n",
      "best score of last 100: 610.0, running reward: 165.35 at episode 604, frame count 310000, time: 7724.448557138443\n",
      "best score of last 100: 610.0, running reward: 167.75 at episode 623, frame count 320000, time: 7991.38206911087\n",
      "best score of last 100: 610.0, running reward: 167.30 at episode 645, frame count 330000, time: 8257.325006961823\n",
      "best score of last 100: 610.0, running reward: 159.10 at episode 667, frame count 340000, time: 8520.046010017395\n",
      "best score of last 100: 550.0, running reward: 155.80 at episode 687, frame count 350000, time: 8783.048645973206\n",
      "best score of last 100: 550.0, running reward: 161.25 at episode 704, frame count 360000, time: 9046.4597260952\n",
      "best score of last 100: 590.0, running reward: 159.00 at episode 724, frame count 370000, time: 9310.978447914124\n",
      "best score of last 100: 590.0, running reward: 166.40 at episode 745, frame count 380000, time: 9575.67848610878\n",
      "best score of last 100: 715.0, running reward: 177.60 at episode 765, frame count 390000, time: 9841.140929937363\n",
      "best score of last 100: 715.0, running reward: 177.45 at episode 785, frame count 400000, time: 10107.275156021118\n",
      "best score of last 100: 775.0, running reward: 185.55 at episode 802, frame count 410000, time: 10374.671869039536\n",
      "best score of last 100: 775.0, running reward: 180.10 at episode 824, frame count 420000, time: 10642.543339967728\n",
      "best score of last 100: 775.0, running reward: 178.95 at episode 843, frame count 430000, time: 10910.612998008728\n",
      "best score of last 100: 775.0, running reward: 173.65 at episode 862, frame count 440000, time: 11179.420185089111\n",
      "best score of last 100: 775.0, running reward: 184.95 at episode 881, frame count 450000, time: 11448.887021064758\n",
      "best score of last 100: 515.0, running reward: 171.55 at episode 899, frame count 460000, time: 11718.775459051132\n",
      "best score of last 100: 515.0, running reward: 175.45 at episode 918, frame count 470000, time: 11993.558056116104\n",
      "best score of last 100: 565.0, running reward: 183.05 at episode 936, frame count 480000, time: 12269.443186044693\n",
      "best score of last 100: 675.0, running reward: 189.45 at episode 953, frame count 490000, time: 12544.60713005066\n",
      "best score of last 100: 675.0, running reward: 189.45 at episode 972, frame count 500000, time: 12820.11620092392\n",
      "best score of last 100: 675.0, running reward: 186.70 at episode 992, frame count 510000, time: 13096.171991109848\n",
      "best score of last 100: 675.0, running reward: 179.35 at episode 1011, frame count 520000, time: 13372.79499411583\n",
      "best score of last 100: 675.0, running reward: 179.70 at episode 1028, frame count 530000, time: 13649.792558908463\n",
      "best score of last 100: 675.0, running reward: 183.85 at episode 1046, frame count 540000, time: 13927.23755812645\n",
      "best score of last 100: 560.0, running reward: 182.50 at episode 1064, frame count 550000, time: 14204.590731143951\n",
      "best score of last 100: 560.0, running reward: 191.85 at episode 1082, frame count 560000, time: 14484.034143924713\n",
      "best score of last 100: 560.0, running reward: 199.85 at episode 1098, frame count 570000, time: 14763.874184131622\n",
      "best score of last 100: 560.0, running reward: 210.50 at episode 1117, frame count 580000, time: 15045.840069055557\n",
      "best score of last 100: 560.0, running reward: 210.00 at episode 1133, frame count 590000, time: 15327.377444028854\n",
      "best score of last 100: 655.0, running reward: 208.20 at episode 1153, frame count 600000, time: 15609.40456700325\n",
      "best score of last 100: 655.0, running reward: 220.05 at episode 1170, frame count 610000, time: 15892.467855930328\n",
      "best score of last 100: 655.0, running reward: 223.70 at episode 1189, frame count 620000, time: 16176.963968992233\n",
      "best score of last 100: 655.0, running reward: 222.85 at episode 1207, frame count 630000, time: 16461.460232019424\n",
      "best score of last 100: 655.0, running reward: 223.00 at episode 1225, frame count 640000, time: 16745.10237312317\n",
      "best score of last 100: 630.0, running reward: 217.45 at episode 1243, frame count 650000, time: 17029.736407995224\n",
      "best score of last 100: 630.0, running reward: 215.05 at episode 1263, frame count 660000, time: 17315.916906118393\n",
      "best score of last 100: 695.0, running reward: 211.90 at episode 1281, frame count 670000, time: 17602.314589977264\n",
      "best score of last 100: 695.0, running reward: 215.10 at episode 1298, frame count 680000, time: 17888.796617031097\n",
      "best score of last 100: 695.0, running reward: 214.20 at episode 1316, frame count 690000, time: 18177.815881967545\n",
      "best score of last 100: 695.0, running reward: 204.15 at episode 1335, frame count 700000, time: 18466.115972042084\n",
      "best score of last 100: 695.0, running reward: 205.10 at episode 1354, frame count 710000, time: 18756.43374800682\n",
      "best score of last 100: 870.0, running reward: 211.20 at episode 1371, frame count 720000, time: 19046.983822107315\n",
      "best score of last 100: 870.0, running reward: 205.35 at episode 1389, frame count 730000, time: 19338.265941143036\n",
      "best score of last 100: 870.0, running reward: 203.05 at episode 1407, frame count 740000, time: 19629.93686914444\n",
      "best score of last 100: 870.0, running reward: 216.15 at episode 1424, frame count 750000, time: 19922.406461000443\n",
      "best score of last 100: 870.0, running reward: 222.85 at episode 1440, frame count 760000, time: 20215.181450128555\n",
      "best score of last 100: 870.0, running reward: 234.25 at episode 1455, frame count 770000, time: 20509.676547050476\n",
      "best score of last 100: 820.0, running reward: 237.90 at episode 1471, frame count 780000, time: 20804.08711194992\n",
      "best score of last 100: 820.0, running reward: 243.60 at episode 1488, frame count 790000, time: 21099.384614944458\n",
      "best score of last 100: 820.0, running reward: 254.75 at episode 1504, frame count 800000, time: 21395.541391134262\n",
      "best score of last 100: 820.0, running reward: 258.75 at episode 1520, frame count 810000, time: 21692.33539891243\n",
      "best score of last 100: 820.0, running reward: 257.55 at episode 1535, frame count 820000, time: 21989.78669309616\n",
      "best score of last 100: 820.0, running reward: 252.05 at episode 1552, frame count 830000, time: 22288.005151987076\n",
      "best score of last 100: 820.0, running reward: 257.00 at episode 1568, frame count 840000, time: 22587.373548030853\n",
      "best score of last 100: 745.0, running reward: 252.30 at episode 1583, frame count 850000, time: 22887.142648935318\n",
      "best score of last 100: 745.0, running reward: 252.15 at episode 1599, frame count 860000, time: 23188.209571123123\n",
      "best score of last 100: 745.0, running reward: 251.05 at episode 1617, frame count 870000, time: 23490.782190084457\n",
      "best score of last 100: 745.0, running reward: 253.85 at episode 1631, frame count 880000, time: 23794.204049110413\n",
      "best score of last 100: 745.0, running reward: 253.90 at episode 1648, frame count 890000, time: 24097.76019191742\n",
      "best score of last 100: 695.0, running reward: 259.75 at episode 1662, frame count 900000, time: 24401.98137497902\n",
      "best score of last 100: 695.0, running reward: 255.80 at episode 1679, frame count 910000, time: 24706.67928004265\n",
      "best score of last 100: 695.0, running reward: 249.90 at episode 1695, frame count 920000, time: 25011.89382791519\n",
      "best score of last 100: 695.0, running reward: 250.40 at episode 1710, frame count 930000, time: 25320.617177963257\n",
      "best score of last 100: 665.0, running reward: 245.20 at episode 1726, frame count 940000, time: 25629.77445411682\n",
      "best score of last 100: 665.0, running reward: 248.95 at episode 1741, frame count 950000, time: 25939.259938001633\n",
      "best score of last 100: 665.0, running reward: 249.15 at episode 1755, frame count 960000, time: 26250.71068406105\n",
      "best score of last 100: 665.0, running reward: 252.95 at episode 1770, frame count 970000, time: 26561.684113025665\n",
      "best score of last 100: 730.0, running reward: 263.55 at episode 1785, frame count 980000, time: 26873.34936594963\n",
      "best score of last 100: 730.0, running reward: 269.60 at episode 1800, frame count 990000, time: 27186.3856010437\n",
      "best score of last 100: 730.0, running reward: 273.85 at episode 1816, frame count 1000000, time: 27500.073382139206\n",
      "best score of last 100: 730.0, running reward: 268.35 at episode 1830, frame count 1010000, time: 27812.700869083405\n",
      "best score of last 100: 730.0, running reward: 271.35 at episode 1845, frame count 1020000, time: 28125.663953065872\n",
      "best score of last 100: 770.0, running reward: 274.45 at episode 1860, frame count 1030000, time: 28438.652184963226\n",
      "best score of last 100: 770.0, running reward: 263.05 at episode 1873, frame count 1040000, time: 28750.734561920166\n",
      "best score of last 100: 770.0, running reward: 269.80 at episode 1888, frame count 1050000, time: 29063.563760995865\n",
      "best score of last 100: 770.0, running reward: 281.00 at episode 1900, frame count 1060000, time: 29376.196810007095\n",
      "best score of last 100: 835.0, running reward: 286.90 at episode 1915, frame count 1070000, time: 29689.58879494667\n",
      "best score of last 100: 835.0, running reward: 286.80 at episode 1930, frame count 1080000, time: 30002.889678955078\n",
      "best score of last 100: 835.0, running reward: 279.85 at episode 1946, frame count 1090000, time: 30316.697379112244\n",
      "best score of last 100: 835.0, running reward: 286.70 at episode 1959, frame count 1100000, time: 30629.738594055176\n",
      "best score of last 100: 835.0, running reward: 279.85 at episode 1976, frame count 1110000, time: 30942.858809947968\n",
      "best score of last 100: 835.0, running reward: 275.25 at episode 1990, frame count 1120000, time: 31255.733444929123\n",
      "best score of last 100: 835.0, running reward: 269.40 at episode 2006, frame count 1130000, time: 31569.24370598793\n",
      "best score of last 100: 725.0, running reward: 258.90 at episode 2021, frame count 1140000, time: 31882.492499113083\n",
      "best score of last 100: 725.0, running reward: 259.75 at episode 2037, frame count 1150000, time: 32195.514194965363\n",
      "best score of last 100: 725.0, running reward: 260.95 at episode 2053, frame count 1160000, time: 32508.947508096695\n",
      "best score of last 100: 725.0, running reward: 253.95 at episode 2067, frame count 1170000, time: 32821.79643702507\n",
      "best score of last 100: 725.0, running reward: 273.75 at episode 2079, frame count 1180000, time: 33133.98056292534\n",
      "best score of last 100: 705.0, running reward: 283.15 at episode 2092, frame count 1190000, time: 33447.80855512619\n",
      "best score of last 100: 705.0, running reward: 289.45 at episode 2107, frame count 1200000, time: 33761.48451113701\n",
      "best score of last 100: 705.0, running reward: 296.45 at episode 2121, frame count 1210000, time: 34075.2506480217\n",
      "best score of last 100: 705.0, running reward: 300.05 at episode 2136, frame count 1220000, time: 34389.43307709694\n",
      "best score of last 100: 705.0, running reward: 304.35 at episode 2149, frame count 1230000, time: 34702.94701504707\n",
      "best score of last 100: 705.0, running reward: 302.60 at episode 2164, frame count 1240000, time: 35016.8887629509\n",
      "best score of last 100: 655.0, running reward: 293.75 at episode 2178, frame count 1250000, time: 35331.019015073776\n",
      "best score of last 100: 655.0, running reward: 289.40 at episode 2194, frame count 1260000, time: 35645.24248504639\n",
      "best score of last 100: 620.0, running reward: 281.70 at episode 2208, frame count 1270000, time: 35959.45404291153\n",
      "best score of last 100: 730.0, running reward: 286.30 at episode 2223, frame count 1280000, time: 36273.65167403221\n",
      "best score of last 100: 750.0, running reward: 292.85 at episode 2235, frame count 1290000, time: 36587.13712000847\n",
      "best score of last 100: 750.0, running reward: 291.80 at episode 2250, frame count 1300000, time: 36901.512665987015\n",
      "best score of last 100: 750.0, running reward: 284.00 at episode 2266, frame count 1310000, time: 37216.12329697609\n",
      "best score of last 100: 750.0, running reward: 285.90 at episode 2280, frame count 1320000, time: 37530.99983596802\n",
      "best score of last 100: 750.0, running reward: 292.55 at episode 2294, frame count 1330000, time: 37845.89750909805\n",
      "best score of last 100: 750.0, running reward: 294.85 at episode 2308, frame count 1340000, time: 38160.4417951107\n",
      "best score of last 100: 750.0, running reward: 287.95 at episode 2322, frame count 1350000, time: 38475.554814100266\n",
      "best score of last 100: 745.0, running reward: 287.45 at episode 2336, frame count 1360000, time: 38789.96611595154\n",
      "best score of last 100: 745.0, running reward: 281.55 at episode 2352, frame count 1370000, time: 39104.97965312004\n",
      "best score of last 100: 745.0, running reward: 293.55 at episode 2367, frame count 1380000, time: 39419.85176610947\n",
      "best score of last 100: 745.0, running reward: 280.95 at episode 2382, frame count 1390000, time: 39735.168643951416\n",
      "best score of last 100: 745.0, running reward: 278.90 at episode 2397, frame count 1400000, time: 40047.97528004646\n",
      "best score of last 100: 745.0, running reward: 282.30 at episode 2411, frame count 1410000, time: 40360.57654809952\n",
      "best score of last 100: 745.0, running reward: 274.10 at episode 2426, frame count 1420000, time: 40672.678873062134\n",
      "best score of last 100: 600.0, running reward: 261.40 at episode 2443, frame count 1430000, time: 40984.75919294357\n",
      "best score of last 100: 590.0, running reward: 263.40 at episode 2458, frame count 1440000, time: 41296.983493089676\n",
      "best score of last 100: 590.0, running reward: 256.70 at episode 2475, frame count 1450000, time: 41610.86952710152\n",
      "best score of last 100: 590.0, running reward: 252.35 at episode 2491, frame count 1460000, time: 41931.47138595581\n",
      "best score of last 100: 590.0, running reward: 253.20 at episode 2508, frame count 1470000, time: 42252.01701903343\n",
      "best score of last 100: 590.0, running reward: 248.65 at episode 2525, frame count 1480000, time: 42575.27114200592\n",
      "best score of last 100: 590.0, running reward: 249.60 at episode 2540, frame count 1490000, time: 42889.45108008385\n",
      "best score of last 100: 650.0, running reward: 253.55 at episode 2555, frame count 1500000, time: 43203.47984004021\n",
      "best score of last 100: 650.0, running reward: 252.50 at episode 2571, frame count 1510000, time: 43516.83212804794\n",
      "best score of last 100: 780.0, running reward: 257.50 at episode 2587, frame count 1520000, time: 43831.09145593643\n",
      "best score of last 100: 780.0, running reward: 265.30 at episode 2602, frame count 1530000, time: 44145.605765104294\n",
      "best score of last 100: 780.0, running reward: 266.10 at episode 2618, frame count 1540000, time: 44460.5682349205\n",
      "best score of last 100: 780.0, running reward: 273.75 at episode 2634, frame count 1550000, time: 44778.00197792053\n",
      "best score of last 100: 780.0, running reward: 270.15 at episode 2649, frame count 1560000, time: 45091.68208193779\n",
      "best score of last 100: 795.0, running reward: 291.25 at episode 2661, frame count 1570000, time: 45406.48337292671\n",
      "best score of last 100: 795.0, running reward: 300.70 at episode 2674, frame count 1580000, time: 45721.669805049896\n",
      "best score of last 100: 795.0, running reward: 301.75 at episode 2688, frame count 1590000, time: 46036.02231693268\n",
      "best score of last 100: 975.0, running reward: 309.05 at episode 2701, frame count 1600000, time: 46350.793909072876\n",
      "best score of last 100: 975.0, running reward: 313.30 at episode 2716, frame count 1610000, time: 46666.56215906143\n",
      "best score of last 100: 975.0, running reward: 313.30 at episode 2732, frame count 1620000, time: 46982.30369091034\n",
      "best score of last 100: 975.0, running reward: 311.55 at episode 2746, frame count 1630000, time: 47296.995232105255\n",
      "best score of last 100: 975.0, running reward: 289.55 at episode 2763, frame count 1640000, time: 47613.313590049744\n",
      "best score of last 100: 975.0, running reward: 286.05 at episode 2777, frame count 1650000, time: 47929.690194129944\n",
      "best score of last 100: 975.0, running reward: 287.75 at episode 2792, frame count 1660000, time: 48245.43115592003\n",
      "best score of last 100: 900.0, running reward: 284.85 at episode 2806, frame count 1670000, time: 48562.25631093979\n",
      "best score of last 100: 900.0, running reward: 286.35 at episode 2821, frame count 1680000, time: 48878.459110975266\n",
      "best score of last 100: 900.0, running reward: 286.80 at episode 2834, frame count 1690000, time: 49193.274017095566\n",
      "best score of last 100: 900.0, running reward: 290.15 at episode 2850, frame count 1700000, time: 49509.38151597977\n",
      "best score of last 100: 900.0, running reward: 287.35 at episode 2866, frame count 1710000, time: 49824.94822001457\n",
      "best score of last 100: 900.0, running reward: 273.50 at episode 2882, frame count 1720000, time: 50139.64329504967\n",
      "best score of last 100: 745.0, running reward: 267.05 at episode 2896, frame count 1730000, time: 50454.77367711067\n",
      "best score of last 100: 745.0, running reward: 274.70 at episode 2911, frame count 1740000, time: 50770.30782914162\n",
      "best score of last 100: 725.0, running reward: 273.50 at episode 2926, frame count 1750000, time: 51085.42813396454\n",
      "best score of last 100: 725.0, running reward: 254.55 at episode 2944, frame count 1760000, time: 51410.75274395943\n",
      "best score of last 100: 725.0, running reward: 259.50 at episode 2960, frame count 1770000, time: 51743.164366960526\n",
      "best score of last 100: 725.0, running reward: 263.10 at episode 2975, frame count 1780000, time: 52057.949468135834\n",
      "best score of last 100: 725.0, running reward: 266.10 at episode 2990, frame count 1790000, time: 52373.78276896477\n",
      "best score of last 100: 750.0, running reward: 268.05 at episode 3003, frame count 1800000, time: 52689.01897192001\n",
      "best score of last 100: 750.0, running reward: 275.15 at episode 3017, frame count 1810000, time: 53003.60400009155\n",
      "best score of last 100: 750.0, running reward: 276.55 at episode 3033, frame count 1820000, time: 53318.032186985016\n",
      "best score of last 100: 750.0, running reward: 277.35 at episode 3049, frame count 1830000, time: 53632.22935295105\n",
      "best score of last 100: 750.0, running reward: 287.55 at episode 3063, frame count 1840000, time: 53946.87474107742\n",
      "best score of last 100: 750.0, running reward: 287.70 at episode 3078, frame count 1850000, time: 54261.23501205444\n",
      "best score of last 100: 750.0, running reward: 282.70 at episode 3093, frame count 1860000, time: 54576.25861001015\n",
      "best score of last 100: 710.0, running reward: 269.90 at episode 3108, frame count 1870000, time: 54891.93831205368\n",
      "best score of last 100: 710.0, running reward: 273.40 at episode 3124, frame count 1880000, time: 55207.49634099007\n",
      "best score of last 100: 635.0, running reward: 279.30 at episode 3139, frame count 1890000, time: 55523.064604997635\n",
      "best score of last 100: 615.0, running reward: 265.05 at episode 3156, frame count 1900000, time: 55838.23989009857\n",
      "best score of last 100: 615.0, running reward: 259.95 at episode 3171, frame count 1910000, time: 56154.39438414574\n",
      "best score of last 100: 615.0, running reward: 265.40 at episode 3186, frame count 1920000, time: 56469.95375204086\n",
      "best score of last 100: 610.0, running reward: 268.35 at episode 3202, frame count 1930000, time: 56785.88061213493\n",
      "best score of last 100: 610.0, running reward: 259.45 at episode 3218, frame count 1940000, time: 57101.243221998215\n",
      "best score of last 100: 715.0, running reward: 257.55 at episode 3234, frame count 1950000, time: 57417.03687310219\n",
      "best score of last 100: 715.0, running reward: 262.40 at episode 3249, frame count 1960000, time: 57733.040163993835\n",
      "best score of last 100: 715.0, running reward: 274.20 at episode 3264, frame count 1970000, time: 58049.15308213234\n",
      "best score of last 100: 715.0, running reward: 260.85 at episode 3280, frame count 1980000, time: 58364.67643713951\n",
      "best score of last 100: 715.0, running reward: 265.25 at episode 3294, frame count 1990000, time: 58680.04558110237\n",
      "best score of last 100: 715.0, running reward: 276.35 at episode 3308, frame count 2000000, time: 58996.82739210129\n",
      "best score of last 100: 720.0, running reward: 280.85 at episode 3323, frame count 2010000, time: 59313.940881967545\n",
      "best score of last 100: 720.0, running reward: 280.90 at episode 3338, frame count 2020000, time: 59630.76290893555\n",
      "best score of last 100: 720.0, running reward: 287.45 at episode 3354, frame count 2030000, time: 59947.77779507637\n",
      "best score of last 100: 745.0, running reward: 284.85 at episode 3368, frame count 2040000, time: 60265.737255096436\n",
      "best score of last 100: 745.0, running reward: 290.90 at episode 3382, frame count 2050000, time: 60583.271704912186\n",
      "best score of last 100: 745.0, running reward: 297.00 at episode 3395, frame count 2060000, time: 60900.80072808266\n",
      "best score of last 100: 745.0, running reward: 310.15 at episode 3408, frame count 2070000, time: 61219.093326091766\n",
      "best score of last 100: 745.0, running reward: 313.40 at episode 3421, frame count 2080000, time: 61537.19321012497\n",
      "best score of last 100: 745.0, running reward: 325.20 at episode 3436, frame count 2090000, time: 61854.62184000015\n",
      "best score of last 100: 745.0, running reward: 327.65 at episode 3451, frame count 2100000, time: 62172.70053792\n",
      "best score of last 100: 720.0, running reward: 321.90 at episode 3465, frame count 2110000, time: 62490.3184530735\n",
      "best score of last 100: 720.0, running reward: 322.80 at episode 3481, frame count 2120000, time: 62807.782141923904\n",
      "best score of last 100: 720.0, running reward: 311.75 at episode 3496, frame count 2130000, time: 63125.28146314621\n",
      "best score of last 100: 610.0, running reward: 294.35 at episode 3510, frame count 2140000, time: 63444.916134119034\n",
      "best score of last 100: 620.0, running reward: 291.60 at episode 3527, frame count 2150000, time: 63768.73887205124\n",
      "best score of last 100: 620.0, running reward: 275.50 at episode 3543, frame count 2160000, time: 64097.571768045425\n",
      "best score of last 100: 620.0, running reward: 274.20 at episode 3560, frame count 2170000, time: 64418.07633113861\n",
      "best score of last 100: 620.0, running reward: 269.75 at episode 3576, frame count 2180000, time: 64736.02810502052\n",
      "best score of last 100: 835.0, running reward: 285.70 at episode 3588, frame count 2190000, time: 65054.249026060104\n",
      "best score of last 100: 835.0, running reward: 292.90 at episode 3600, frame count 2200000, time: 65372.49891805649\n",
      "best score of last 100: 835.0, running reward: 299.75 at episode 3613, frame count 2210000, time: 65691.26759696007\n",
      "best score of last 100: 835.0, running reward: 300.80 at episode 3628, frame count 2220000, time: 66009.01032400131\n",
      "best score of last 100: 835.0, running reward: 308.35 at episode 3644, frame count 2230000, time: 66328.16571593285\n",
      "best score of last 100: 835.0, running reward: 313.25 at episode 3658, frame count 2240000, time: 66646.89262700081\n",
      "best score of last 100: 835.0, running reward: 321.00 at episode 3671, frame count 2250000, time: 66964.84604597092\n",
      "best score of last 100: 800.0, running reward: 311.20 at episode 3686, frame count 2260000, time: 67283.50984811783\n",
      "best score of last 100: 800.0, running reward: 292.50 at episode 3703, frame count 2270000, time: 67602.36340999603\n",
      "best score of last 100: 800.0, running reward: 289.95 at episode 3716, frame count 2280000, time: 67920.1258149147\n",
      "best score of last 100: 800.0, running reward: 296.30 at episode 3728, frame count 2290000, time: 68238.39365792274\n",
      "best score of last 100: 695.0, running reward: 301.10 at episode 3741, frame count 2300000, time: 68555.89099693298\n",
      "best score of last 100: 695.0, running reward: 306.05 at episode 3756, frame count 2310000, time: 68874.55372309685\n",
      "best score of last 100: 750.0, running reward: 313.70 at episode 3769, frame count 2320000, time: 69193.51136612892\n",
      "best score of last 100: 750.0, running reward: 317.45 at episode 3783, frame count 2330000, time: 69511.99874711037\n",
      "best score of last 100: 750.0, running reward: 323.75 at episode 3795, frame count 2340000, time: 69833.48397397995\n",
      "best score of last 100: 750.0, running reward: 329.25 at episode 3808, frame count 2350000, time: 70150.44764709473\n",
      "best score of last 100: 750.0, running reward: 321.75 at episode 3823, frame count 2360000, time: 70468.39209294319\n",
      "best score of last 100: 750.0, running reward: 308.60 at episode 3838, frame count 2370000, time: 70786.80437111855\n",
      "best score of last 100: 750.0, running reward: 305.25 at episode 3854, frame count 2380000, time: 71104.0882089138\n",
      "best score of last 100: 835.0, running reward: 306.15 at episode 3867, frame count 2390000, time: 71432.71791005135\n",
      "best score of last 100: 835.0, running reward: 309.40 at episode 3882, frame count 2400000, time: 71799.62015104294\n",
      "best score of last 100: 835.0, running reward: 298.90 at episode 3898, frame count 2410000, time: 72241.18970513344\n",
      "best score of last 100: 835.0, running reward: 295.10 at episode 3913, frame count 2420000, time: 72682.96830797195\n",
      "best score of last 100: 835.0, running reward: 296.10 at episode 3928, frame count 2430000, time: 73118.56655311584\n",
      "best score of last 100: 835.0, running reward: 300.55 at episode 3942, frame count 2440000, time: 73560.4196100235\n",
      "best score of last 100: 835.0, running reward: 299.05 at episode 3957, frame count 2450000, time: 73901.71141695976\n",
      "best score of last 100: 630.0, running reward: 290.80 at episode 3971, frame count 2460000, time: 74229.65167999268\n",
      "best score of last 100: 730.0, running reward: 298.55 at episode 3984, frame count 2470000, time: 74558.33965706825\n",
      "best score of last 100: 730.0, running reward: 296.20 at episode 4000, frame count 2480000, time: 74904.49669504166\n",
      "best score of last 100: 1085.0, running reward: 305.70 at episode 4014, frame count 2490000, time: 75251.38185596466\n",
      "best score of last 100: 1085.0, running reward: 316.35 at episode 4028, frame count 2500000, time: 75590.82876610756\n",
      "best score of last 100: 1085.0, running reward: 315.95 at episode 4041, frame count 2510000, time: 75920.2538330555\n",
      "best score of last 100: 1085.0, running reward: 310.15 at episode 4057, frame count 2520000, time: 76242.15807700157\n",
      "best score of last 100: 1085.0, running reward: 301.35 at episode 4072, frame count 2530000, time: 76562.86618900299\n",
      "best score of last 100: 1085.0, running reward: 298.60 at episode 4085, frame count 2540000, time: 76884.19756603241\n",
      "best score of last 100: 1085.0, running reward: 305.10 at episode 4098, frame count 2550000, time: 77205.48484706879\n",
      "best score of last 100: 675.0, running reward: 308.90 at episode 4111, frame count 2560000, time: 77527.15129899979\n",
      "best score of last 100: 675.0, running reward: 301.75 at episode 4126, frame count 2570000, time: 77849.49195814133\n",
      "best score of last 100: 635.0, running reward: 295.35 at episode 4141, frame count 2580000, time: 78171.10711598396\n",
      "best score of last 100: 635.0, running reward: 301.65 at episode 4155, frame count 2590000, time: 78493.72000813484\n",
      "best score of last 100: 910.0, running reward: 306.10 at episode 4168, frame count 2600000, time: 78818.57827091217\n",
      "best score of last 100: 980.0, running reward: 330.25 at episode 4180, frame count 2610000, time: 79160.03845191002\n",
      "best score of last 100: 980.0, running reward: 336.55 at episode 4192, frame count 2620000, time: 79507.4647910595\n",
      "best score of last 100: 980.0, running reward: 336.70 at episode 4206, frame count 2630000, time: 79866.93561792374\n",
      "best score of last 100: 980.0, running reward: 341.60 at episode 4219, frame count 2640000, time: 80224.4138109684\n",
      "best score of last 100: 980.0, running reward: 351.15 at episode 4233, frame count 2650000, time: 80585.45566701889\n",
      "best score of last 100: 980.0, running reward: 360.15 at episode 4246, frame count 2660000, time: 80934.17234611511\n",
      "best score of last 100: 980.0, running reward: 362.55 at episode 4262, frame count 2670000, time: 81256.81811714172\n",
      "best score of last 100: 755.0, running reward: 354.35 at episode 4275, frame count 2680000, time: 81579.68225002289\n",
      "best score of last 100: 790.0, running reward: 348.70 at episode 4287, frame count 2690000, time: 81902.10398507118\n",
      "best score of last 100: 790.0, running reward: 341.40 at episode 4300, frame count 2700000, time: 82224.82722902298\n",
      "best score of last 100: 790.0, running reward: 344.75 at episode 4315, frame count 2710000, time: 82549.22492003441\n",
      "best score of last 100: 790.0, running reward: 332.55 at episode 4330, frame count 2720000, time: 82872.23139810562\n",
      "best score of last 100: 790.0, running reward: 323.45 at episode 4344, frame count 2730000, time: 83194.93091392517\n",
      "best score of last 100: 1040.0, running reward: 324.55 at episode 4361, frame count 2740000, time: 83518.08945798874\n",
      "best score of last 100: 1040.0, running reward: 315.85 at episode 4375, frame count 2750000, time: 83841.33358502388\n",
      "best score of last 100: 1040.0, running reward: 303.20 at episode 4390, frame count 2760000, time: 84164.86171007156\n",
      "best score of last 100: 1040.0, running reward: 304.00 at episode 4401, frame count 2770000, time: 84488.43016195297\n",
      "best score of last 100: 1040.0, running reward: 308.30 at episode 4414, frame count 2780000, time: 84813.23283910751\n",
      "best score of last 100: 1040.0, running reward: 320.10 at episode 4428, frame count 2790000, time: 85144.4652171135\n",
      "best score of last 100: 1040.0, running reward: 319.55 at episode 4441, frame count 2800000, time: 85472.91338801384\n",
      "best score of last 100: 770.0, running reward: 322.70 at episode 4453, frame count 2810000, time: 85797.76411795616\n",
      "best score of last 100: 830.0, running reward: 340.65 at episode 4467, frame count 2820000, time: 86124.00049591064\n",
      "best score of last 100: 945.0, running reward: 342.70 at episode 4478, frame count 2830000, time: 86447.98524308205\n",
      "best score of last 100: 945.0, running reward: 341.05 at episode 4493, frame count 2840000, time: 86773.77437210083\n",
      "best score of last 100: 945.0, running reward: 334.20 at episode 4507, frame count 2850000, time: 87099.31564593315\n",
      "best score of last 100: 945.0, running reward: 338.65 at episode 4519, frame count 2860000, time: 87435.27345395088\n",
      "best score of last 100: 945.0, running reward: 336.95 at episode 4534, frame count 2870000, time: 87767.26049304008\n",
      "best score of last 100: 945.0, running reward: 351.05 at episode 4544, frame count 2880000, time: 88092.18053603172\n",
      "best score of last 100: 945.0, running reward: 344.85 at episode 4558, frame count 2890000, time: 88416.74391293526\n",
      "best score of last 100: 945.0, running reward: 349.05 at episode 4570, frame count 2900000, time: 88741.7571709156\n",
      "best score of last 100: 770.0, running reward: 343.40 at episode 4584, frame count 2910000, time: 89071.7624680996\n",
      "best score of last 100: 770.0, running reward: 355.70 at episode 4598, frame count 2920000, time: 89424.33765602112\n",
      "best score of last 100: 770.0, running reward: 353.00 at episode 4612, frame count 2930000, time: 89771.10033106804\n",
      "best score of last 100: 770.0, running reward: 351.65 at episode 4626, frame count 2940000, time: 90109.80648803711\n",
      "best score of last 100: 800.0, running reward: 344.80 at episode 4641, frame count 2950000, time: 90449.32689404488\n",
      "best score of last 100: 820.0, running reward: 343.45 at episode 4653, frame count 2960000, time: 90780.98395395279\n",
      "best score of last 100: 820.0, running reward: 332.15 at episode 4667, frame count 2970000, time: 91108.16482496262\n",
      "best score of last 100: 820.0, running reward: 329.45 at episode 4681, frame count 2980000, time: 91435.61723208427\n",
      "best score of last 100: 820.0, running reward: 325.30 at episode 4695, frame count 2990000, time: 91762.96099996567\n",
      "best score of last 100: 820.0, running reward: 319.00 at episode 4711, frame count 3000000, time: 92091.1516919136\n",
      "best score of last 100: 820.0, running reward: 321.85 at episode 4724, frame count 3010000, time: 92417.65647602081\n",
      "best score of last 100: 820.0, running reward: 311.35 at episode 4741, frame count 3020000, time: 92745.18493795395\n",
      "best score of last 100: 700.0, running reward: 307.55 at episode 4755, frame count 3030000, time: 93072.40980410576\n",
      "best score of last 100: 715.0, running reward: 308.05 at episode 4770, frame count 3040000, time: 93399.95732212067\n",
      "best score of last 100: 1010.0, running reward: 312.80 at episode 4782, frame count 3050000, time: 93727.26780414581\n",
      "best score of last 100: 1010.0, running reward: 313.80 at episode 4797, frame count 3060000, time: 94056.5370399952\n",
      "best score of last 100: 1010.0, running reward: 320.20 at episode 4810, frame count 3070000, time: 94384.57553601265\n",
      "best score of last 100: 1010.0, running reward: 319.25 at episode 4825, frame count 3080000, time: 94711.47792196274\n",
      "best score of last 100: 1010.0, running reward: 327.90 at episode 4838, frame count 3090000, time: 95039.6954729557\n",
      "best score of last 100: 1010.0, running reward: 343.15 at episode 4850, frame count 3100000, time: 95366.8248770237\n",
      "best score of last 100: 1010.0, running reward: 327.35 at episode 4865, frame count 3110000, time: 95697.12887001038\n",
      "best score of last 100: 775.0, running reward: 315.75 at episode 4880, frame count 3120000, time: 96025.68396091461\n",
      "best score of last 100: 775.0, running reward: 323.20 at episode 4893, frame count 3130000, time: 96353.97722792625\n",
      "best score of last 100: 765.0, running reward: 321.45 at episode 4907, frame count 3140000, time: 96682.56351995468\n",
      "best score of last 100: 725.0, running reward: 311.25 at episode 4923, frame count 3150000, time: 97010.13052511215\n",
      "best score of last 100: 725.0, running reward: 310.90 at episode 4935, frame count 3160000, time: 97337.59359693527\n",
      "best score of last 100: 725.0, running reward: 292.30 at episode 4952, frame count 3170000, time: 97665.18258094788\n",
      "best score of last 100: 725.0, running reward: 290.60 at episode 4968, frame count 3180000, time: 97992.970318079\n",
      "best score of last 100: 725.0, running reward: 296.20 at episode 4981, frame count 3190000, time: 98320.5114710331\n",
      "best score of last 100: 720.0, running reward: 291.55 at episode 4995, frame count 3200000, time: 98648.29919600487\n",
      "best score of last 100: 720.0, running reward: 284.20 at episode 5010, frame count 3210000, time: 98976.18493008614\n",
      "best score of last 100: 730.0, running reward: 301.45 at episode 5024, frame count 3220000, time: 99303.2418050766\n",
      "best score of last 100: 730.0, running reward: 288.85 at episode 5040, frame count 3230000, time: 99631.14519095421\n",
      "best score of last 100: 730.0, running reward: 300.55 at episode 5054, frame count 3240000, time: 99958.58994197845\n",
      "best score of last 100: 730.0, running reward: 309.55 at episode 5068, frame count 3250000, time: 100285.82396292686\n",
      "best score of last 100: 730.0, running reward: 308.15 at episode 5082, frame count 3260000, time: 100613.0971069336\n",
      "best score of last 100: 815.0, running reward: 310.95 at episode 5096, frame count 3270000, time: 100940.80818390846\n",
      "best score of last 100: 815.0, running reward: 312.45 at episode 5112, frame count 3280000, time: 101269.84441304207\n",
      "best score of last 100: 815.0, running reward: 301.30 at episode 5128, frame count 3290000, time: 101598.65936112404\n",
      "best score of last 100: 815.0, running reward: 304.05 at episode 5144, frame count 3300000, time: 101926.62171697617\n",
      "best score of last 100: 815.0, running reward: 295.45 at episode 5159, frame count 3310000, time: 102254.52159309387\n",
      "best score of last 100: 815.0, running reward: 293.70 at episode 5173, frame count 3320000, time: 102582.89764094353\n",
      "best score of last 100: 815.0, running reward: 295.30 at episode 5186, frame count 3330000, time: 102910.77890896797\n",
      "best score of last 100: 885.0, running reward: 297.55 at episode 5198, frame count 3340000, time: 103238.23102211952\n",
      "best score of last 100: 885.0, running reward: 301.30 at episode 5213, frame count 3350000, time: 103567.47937893867\n",
      "best score of last 100: 885.0, running reward: 299.10 at episode 5229, frame count 3360000, time: 103897.50123596191\n",
      "best score of last 100: 905.0, running reward: 309.05 at episode 5244, frame count 3370000, time: 104226.97888112068\n",
      "best score of last 100: 905.0, running reward: 321.30 at episode 5257, frame count 3380000, time: 104556.18810200691\n",
      "best score of last 100: 905.0, running reward: 321.80 at episode 5271, frame count 3390000, time: 104886.03584694862\n",
      "best score of last 100: 905.0, running reward: 327.65 at episode 5283, frame count 3400000, time: 105215.17884707451\n",
      "best score of last 100: 905.0, running reward: 323.90 at episode 5294, frame count 3410000, time: 105544.74098396301\n",
      "best score of last 100: 905.0, running reward: 323.95 at episode 5310, frame count 3420000, time: 105874.98037195206\n",
      "best score of last 100: 905.0, running reward: 321.80 at episode 5325, frame count 3430000, time: 106205.07494997978\n",
      "best score of last 100: 1000.0, running reward: 317.30 at episode 5341, frame count 3440000, time: 106535.28529691696\n",
      "best score of last 100: 1000.0, running reward: 314.25 at episode 5358, frame count 3450000, time: 106865.66555595398\n",
      "best score of last 100: 1000.0, running reward: 308.55 at episode 5373, frame count 3460000, time: 107194.87970614433\n",
      "best score of last 100: 1000.0, running reward: 294.15 at episode 5388, frame count 3470000, time: 107525.10714411736\n",
      "best score of last 100: 1000.0, running reward: 298.00 at episode 5403, frame count 3480000, time: 107855.62470006943\n",
      "best score of last 100: 1000.0, running reward: 307.85 at episode 5418, frame count 3490000, time: 108185.42100811005\n",
      "best score of last 100: 970.0, running reward: 302.00 at episode 5432, frame count 3500000, time: 108515.66200208664\n",
      "best score of last 100: 970.0, running reward: 307.75 at episode 5447, frame count 3510000, time: 108845.87491893768\n",
      "best score of last 100: 970.0, running reward: 321.35 at episode 5462, frame count 3520000, time: 109176.009786129\n",
      "best score of last 100: 970.0, running reward: 315.20 at episode 5476, frame count 3530000, time: 109506.81258106232\n",
      "best score of last 100: 970.0, running reward: 313.30 at episode 5491, frame count 3540000, time: 109837.42448306084\n",
      "best score of last 100: 665.0, running reward: 302.75 at episode 5506, frame count 3550000, time: 110167.68390107155\n",
      "best score of last 100: 665.0, running reward: 297.80 at episode 5522, frame count 3560000, time: 110498.07548594475\n",
      "best score of last 100: 755.0, running reward: 307.15 at episode 5535, frame count 3570000, time: 110827.43839907646\n",
      "best score of last 100: 765.0, running reward: 309.00 at episode 5548, frame count 3580000, time: 111157.86533093452\n",
      "best score of last 100: 765.0, running reward: 306.70 at episode 5562, frame count 3590000, time: 111487.5760769844\n",
      "best score of last 100: 765.0, running reward: 309.75 at episode 5576, frame count 3600000, time: 111817.85166811943\n",
      "best score of last 100: 765.0, running reward: 310.40 at episode 5589, frame count 3610000, time: 112147.4005150795\n",
      "best score of last 100: 805.0, running reward: 324.60 at episode 5601, frame count 3620000, time: 112477.62767410278\n",
      "best score of last 100: 805.0, running reward: 325.50 at episode 5617, frame count 3630000, time: 112810.69917201996\n",
      "best score of last 100: 830.0, running reward: 327.50 at episode 5632, frame count 3640000, time: 113143.00956392288\n",
      "best score of last 100: 830.0, running reward: 320.80 at episode 5646, frame count 3650000, time: 113476.32296800613\n",
      "best score of last 100: 830.0, running reward: 314.70 at episode 5661, frame count 3660000, time: 113809.75747704506\n",
      "best score of last 100: 830.0, running reward: 317.45 at episode 5673, frame count 3670000, time: 114141.44316411018\n",
      "best score of last 100: 830.0, running reward: 318.80 at episode 5689, frame count 3680000, time: 114473.65906000137\n",
      "best score of last 100: 830.0, running reward: 312.70 at episode 5703, frame count 3690000, time: 114805.44232201576\n",
      "best score of last 100: 830.0, running reward: 316.60 at episode 5717, frame count 3700000, time: 115137.21820998192\n",
      "best score of last 100: 830.0, running reward: 324.80 at episode 5729, frame count 3710000, time: 115469.71169400215\n",
      "best score of last 100: 695.0, running reward: 319.25 at episode 5744, frame count 3720000, time: 115800.89662909508\n",
      "best score of last 100: 695.0, running reward: 307.25 at episode 5761, frame count 3730000, time: 116134.03455209732\n",
      "best score of last 100: 635.0, running reward: 303.95 at episode 5777, frame count 3740000, time: 116465.90239191055\n",
      "best score of last 100: 610.0, running reward: 298.30 at episode 5792, frame count 3750000, time: 116798.19433999062\n",
      "best score of last 100: 640.0, running reward: 294.85 at episode 5808, frame count 3760000, time: 117129.49016714096\n",
      "best score of last 100: 675.0, running reward: 286.95 at episode 5824, frame count 3770000, time: 117460.68396997452\n",
      "best score of last 100: 675.0, running reward: 284.05 at episode 5839, frame count 3780000, time: 117792.35324811935\n",
      "best score of last 100: 730.0, running reward: 299.95 at episode 5851, frame count 3790000, time: 118123.3838660717\n",
      "best score of last 100: 730.0, running reward: 308.75 at episode 5867, frame count 3800000, time: 118455.00725007057\n",
      "best score of last 100: 730.0, running reward: 319.95 at episode 5882, frame count 3810000, time: 118786.58066701889\n",
      "best score of last 100: 800.0, running reward: 329.50 at episode 5896, frame count 3820000, time: 119117.56977200508\n",
      "best score of last 100: 800.0, running reward: 324.85 at episode 5912, frame count 3830000, time: 119449.01825094223\n",
      "best score of last 100: 800.0, running reward: 329.30 at episode 5927, frame count 3840000, time: 119779.39815998077\n",
      "best score of last 100: 800.0, running reward: 325.85 at episode 5941, frame count 3850000, time: 120109.3302321434\n",
      "best score of last 100: 800.0, running reward: 322.40 at episode 5954, frame count 3860000, time: 120439.71433997154\n",
      "best score of last 100: 800.0, running reward: 324.60 at episode 5968, frame count 3870000, time: 120770.75211191177\n",
      "best score of last 100: 800.0, running reward: 318.45 at episode 5984, frame count 3880000, time: 121101.51254701614\n",
      "best score of last 100: 805.0, running reward: 318.10 at episode 5999, frame count 3890000, time: 121431.54078292847\n",
      "best score of last 100: 805.0, running reward: 321.20 at episode 6013, frame count 3900000, time: 121766.05670809746\n",
      "best score of last 100: 805.0, running reward: 318.55 at episode 6027, frame count 3910000, time: 122100.45756912231\n",
      "best score of last 100: 805.0, running reward: 323.95 at episode 6041, frame count 3920000, time: 122435.42700004578\n",
      "best score of last 100: 805.0, running reward: 324.05 at episode 6054, frame count 3930000, time: 122770.07608699799\n",
      "best score of last 100: 805.0, running reward: 338.60 at episode 6067, frame count 3940000, time: 123104.04769992828\n",
      "best score of last 100: 805.0, running reward: 333.85 at episode 6084, frame count 3950000, time: 123437.93664908409\n",
      "best score of last 100: 760.0, running reward: 333.25 at episode 6098, frame count 3960000, time: 123771.86108613014\n",
      "best score of last 100: 760.0, running reward: 333.00 at episode 6112, frame count 3970000, time: 124104.21460103989\n",
      "best score of last 100: 760.0, running reward: 346.95 at episode 6124, frame count 3980000, time: 124436.07454013824\n",
      "best score of last 100: 760.0, running reward: 339.50 at episode 6139, frame count 3990000, time: 124767.95260095596\n",
      "best score of last 100: 760.0, running reward: 336.90 at episode 6152, frame count 4000000, time: 125100.54031014442\n",
      "best score of last 100: 850.0, running reward: 330.50 at episode 6166, frame count 4010000, time: 125433.08729791641\n",
      "best score of last 100: 850.0, running reward: 322.90 at episode 6183, frame count 4020000, time: 125765.25355100632\n",
      "best score of last 100: 850.0, running reward: 314.50 at episode 6200, frame count 4030000, time: 126097.02040290833\n",
      "best score of last 100: 955.0, running reward: 325.90 at episode 6212, frame count 4040000, time: 126428.79952597618\n",
      "best score of last 100: 955.0, running reward: 304.35 at episode 6229, frame count 4050000, time: 126761.22407507896\n",
      "best score of last 100: 955.0, running reward: 312.15 at episode 6242, frame count 4060000, time: 127093.08103609085\n",
      "best score of last 100: 955.0, running reward: 301.45 at episode 6256, frame count 4070000, time: 127425.73760414124\n",
      "best score of last 100: 955.0, running reward: 299.75 at episode 6272, frame count 4080000, time: 127758.4874689579\n",
      "best score of last 100: 955.0, running reward: 318.95 at episode 6283, frame count 4090000, time: 128090.89031004906\n",
      "best score of last 100: 955.0, running reward: 331.40 at episode 6298, frame count 4100000, time: 128424.07904410362\n",
      "best score of last 100: 805.0, running reward: 319.40 at episode 6313, frame count 4110000, time: 128756.01054406166\n",
      "best score of last 100: 805.0, running reward: 335.35 at episode 6325, frame count 4120000, time: 129088.50971913338\n",
      "best score of last 100: 805.0, running reward: 332.70 at episode 6340, frame count 4130000, time: 129430.57610702515\n",
      "best score of last 100: 805.0, running reward: 337.45 at episode 6353, frame count 4140000, time: 129774.53483009338\n",
      "best score of last 100: 805.0, running reward: 342.40 at episode 6368, frame count 4150000, time: 130129.51053214073\n",
      "best score of last 100: 625.0, running reward: 315.30 at episode 6385, frame count 4160000, time: 130485.18224191666\n",
      "best score of last 100: 625.0, running reward: 312.60 at episode 6400, frame count 4170000, time: 130849.84324502945\n",
      "best score of last 100: 625.0, running reward: 322.90 at episode 6413, frame count 4180000, time: 131203.48002791405\n",
      "best score of last 100: 710.0, running reward: 304.65 at episode 6430, frame count 4190000, time: 131546.16840291023\n",
      "best score of last 100: 710.0, running reward: 311.90 at episode 6442, frame count 4200000, time: 131885.68542099\n",
      "best score of last 100: 710.0, running reward: 318.35 at episode 6457, frame count 4210000, time: 132224.7339041233\n",
      "best score of last 100: 710.0, running reward: 315.50 at episode 6470, frame count 4220000, time: 132563.73144602776\n",
      "best score of last 100: 710.0, running reward: 332.40 at episode 6483, frame count 4230000, time: 132914.64005303383\n",
      "best score of last 100: 870.0, running reward: 346.35 at episode 6497, frame count 4240000, time: 133265.78818511963\n",
      "best score of last 100: 870.0, running reward: 340.90 at episode 6511, frame count 4250000, time: 133606.7838730812\n",
      "best score of last 100: 870.0, running reward: 343.75 at episode 6523, frame count 4260000, time: 133941.0848491192\n",
      "best score of last 100: 870.0, running reward: 360.10 at episode 6535, frame count 4270000, time: 134282.58937096596\n",
      "best score of last 100: 870.0, running reward: 343.90 at episode 6552, frame count 4280000, time: 134621.41783308983\n",
      "best score of last 100: 870.0, running reward: 361.70 at episode 6564, frame count 4290000, time: 134950.49223709106\n",
      "best score of last 100: 870.0, running reward: 353.65 at episode 6577, frame count 4300000, time: 135279.21470308304\n",
      "best score of last 100: 870.0, running reward: 354.05 at episode 6591, frame count 4310000, time: 135609.5008149147\n",
      "best score of last 100: 825.0, running reward: 355.00 at episode 6601, frame count 4320000, time: 135942.4069559574\n",
      "best score of last 100: 860.0, running reward: 366.15 at episode 6615, frame count 4330000, time: 136278.37759113312\n",
      "best score of last 100: 860.0, running reward: 373.55 at episode 6627, frame count 4340000, time: 136614.73175096512\n",
      "best score of last 100: 860.0, running reward: 366.85 at episode 6640, frame count 4350000, time: 136950.5886940956\n",
      "best score of last 100: 860.0, running reward: 380.50 at episode 6653, frame count 4360000, time: 137288.84906506538\n",
      "best score of last 100: 860.0, running reward: 375.95 at episode 6666, frame count 4370000, time: 137623.81073713303\n",
      "best score of last 100: 860.0, running reward: 372.35 at episode 6681, frame count 4380000, time: 137958.41947293282\n",
      "best score of last 100: 860.0, running reward: 373.10 at episode 6695, frame count 4390000, time: 138294.06423807144\n",
      "best score of last 100: 845.0, running reward: 354.70 at episode 6708, frame count 4400000, time: 138628.82351708412\n",
      "best score of last 100: 845.0, running reward: 355.90 at episode 6722, frame count 4410000, time: 138961.47091293335\n",
      "best score of last 100: 845.0, running reward: 345.95 at episode 6736, frame count 4420000, time: 139293.15892100334\n",
      "best score of last 100: 845.0, running reward: 351.70 at episode 6749, frame count 4430000, time: 139625.06668806076\n",
      "best score of last 100: 845.0, running reward: 350.05 at episode 6763, frame count 4440000, time: 139957.2334740162\n",
      "best score of last 100: 760.0, running reward: 348.95 at episode 6777, frame count 4450000, time: 140288.32623910904\n",
      "best score of last 100: 760.0, running reward: 353.75 at episode 6790, frame count 4460000, time: 140621.23689103127\n",
      "best score of last 100: 760.0, running reward: 339.10 at episode 6807, frame count 4470000, time: 140953.878813982\n",
      "best score of last 100: 995.0, running reward: 355.35 at episode 6818, frame count 4480000, time: 141285.07571291924\n",
      "best score of last 100: 995.0, running reward: 355.70 at episode 6832, frame count 4490000, time: 141617.18133592606\n",
      "best score of last 100: 995.0, running reward: 345.85 at episode 6848, frame count 4500000, time: 141949.10808491707\n",
      "best score of last 100: 995.0, running reward: 348.50 at episode 6860, frame count 4510000, time: 142279.92363405228\n",
      "best score of last 100: 995.0, running reward: 343.60 at episode 6874, frame count 4520000, time: 142611.6020720005\n",
      "best score of last 100: 995.0, running reward: 330.65 at episode 6890, frame count 4530000, time: 142943.27666306496\n",
      "best score of last 100: 995.0, running reward: 338.50 at episode 6902, frame count 4540000, time: 143274.23913908005\n",
      "best score of last 100: 905.0, running reward: 344.00 at episode 6916, frame count 4550000, time: 143604.23532295227\n",
      "best score of last 100: 905.0, running reward: 342.60 at episode 6931, frame count 4560000, time: 143935.9811589718\n",
      "best score of last 100: 905.0, running reward: 339.00 at episode 6946, frame count 4570000, time: 144266.60719108582\n",
      "best score of last 100: 905.0, running reward: 343.35 at episode 6962, frame count 4580000, time: 144597.3448419571\n",
      "best score of last 100: 905.0, running reward: 344.00 at episode 6975, frame count 4590000, time: 144932.54760313034\n",
      "best score of last 100: 905.0, running reward: 356.80 at episode 6988, frame count 4600000, time: 145264.56690096855\n",
      "best score of last 100: 905.0, running reward: 354.60 at episode 7001, frame count 4610000, time: 145597.1941189766\n",
      "best score of last 100: 905.0, running reward: 351.00 at episode 7014, frame count 4620000, time: 145930.19848394394\n",
      "best score of last 100: 860.0, running reward: 341.25 at episode 7028, frame count 4630000, time: 146262.75084590912\n",
      "best score of last 100: 860.0, running reward: 346.30 at episode 7041, frame count 4640000, time: 146595.55716514587\n",
      "best score of last 100: 1260.0, running reward: 355.35 at episode 7053, frame count 4650000, time: 146928.52427506447\n",
      "best score of last 100: 1260.0, running reward: 361.25 at episode 7065, frame count 4660000, time: 147260.94964504242\n",
      "best score of last 100: 1260.0, running reward: 339.30 at episode 7083, frame count 4670000, time: 147594.2163579464\n",
      "best score of last 100: 1260.0, running reward: 346.80 at episode 7094, frame count 4680000, time: 147928.16677212715\n",
      "best score of last 100: 1260.0, running reward: 342.45 at episode 7108, frame count 4690000, time: 148264.55942702293\n",
      "best score of last 100: 1260.0, running reward: 362.20 at episode 7120, frame count 4700000, time: 148614.57040596008\n",
      "best score of last 100: 1260.0, running reward: 356.55 at episode 7136, frame count 4710000, time: 148948.0046620369\n",
      "best score of last 100: 1145.0, running reward: 347.05 at episode 7152, frame count 4720000, time: 149304.84119701385\n",
      "best score of last 100: 1145.0, running reward: 342.30 at episode 7168, frame count 4730000, time: 149641.84609007835\n",
      "best score of last 100: 1145.0, running reward: 357.75 at episode 7180, frame count 4740000, time: 149978.30365300179\n",
      "best score of last 100: 1135.0, running reward: 362.40 at episode 7193, frame count 4750000, time: 150314.32615113258\n",
      "best score of last 100: 1135.0, running reward: 362.60 at episode 7207, frame count 4760000, time: 150650.9320681095\n",
      "best score of last 100: 1135.0, running reward: 340.90 at episode 7221, frame count 4770000, time: 150992.21101999283\n",
      "best score of last 100: 1135.0, running reward: 339.55 at episode 7237, frame count 4780000, time: 151322.76538610458\n",
      "best score of last 100: 1135.0, running reward: 350.15 at episode 7250, frame count 4790000, time: 151653.14672899246\n",
      "best score of last 100: 1135.0, running reward: 347.45 at episode 7265, frame count 4800000, time: 151984.6946659088\n",
      "best score of last 100: 1135.0, running reward: 346.70 at episode 7280, frame count 4810000, time: 152316.2029979229\n",
      "best score of last 100: 940.0, running reward: 346.85 at episode 7292, frame count 4820000, time: 152648.12689113617\n",
      "best score of last 100: 940.0, running reward: 352.75 at episode 7305, frame count 4830000, time: 152981.89588212967\n",
      "best score of last 100: 940.0, running reward: 366.85 at episode 7318, frame count 4840000, time: 153317.12332606316\n",
      "best score of last 100: 940.0, running reward: 369.10 at episode 7333, frame count 4850000, time: 153651.91984009743\n",
      "best score of last 100: 940.0, running reward: 378.75 at episode 7346, frame count 4860000, time: 153985.6273379326\n",
      "best score of last 100: 940.0, running reward: 376.45 at episode 7361, frame count 4870000, time: 154319.5689251423\n",
      "best score of last 100: 940.0, running reward: 367.70 at episode 7377, frame count 4880000, time: 154652.27086305618\n",
      "best score of last 100: 765.0, running reward: 337.05 at episode 7394, frame count 4890000, time: 154984.98363900185\n",
      "best score of last 100: 765.0, running reward: 333.65 at episode 7410, frame count 4900000, time: 155314.5941529274\n",
      "best score of last 100: 745.0, running reward: 332.05 at episode 7425, frame count 4910000, time: 155643.10635900497\n",
      "best score of last 100: 745.0, running reward: 324.55 at episode 7440, frame count 4920000, time: 155972.70756697655\n",
      "best score of last 100: 675.0, running reward: 330.60 at episode 7452, frame count 4930000, time: 156302.4748620987\n",
      "best score of last 100: 675.0, running reward: 325.40 at episode 7468, frame count 4940000, time: 156632.26134490967\n",
      "best score of last 100: 675.0, running reward: 339.10 at episode 7481, frame count 4950000, time: 156962.24484491348\n",
      "best score of last 100: 830.0, running reward: 354.15 at episode 7495, frame count 4960000, time: 157291.22387099266\n",
      "best score of last 100: 830.0, running reward: 363.80 at episode 7507, frame count 4970000, time: 157621.01842093468\n",
      "best score of last 100: 830.0, running reward: 356.70 at episode 7522, frame count 4980000, time: 157949.4627211094\n",
      "best score of last 100: 830.0, running reward: 357.00 at episode 7536, frame count 4990000, time: 158277.9981970787\n",
      "best score of last 100: 830.0, running reward: 362.70 at episode 7548, frame count 5000000, time: 158607.13496398926\n",
      "best score of last 100: 830.0, running reward: 354.00 at episode 7564, frame count 5010000, time: 158937.09948205948\n",
      "best score of last 100: 830.0, running reward: 338.20 at episode 7578, frame count 5020000, time: 159266.73316407204\n",
      "best score of last 100: 945.0, running reward: 334.45 at episode 7593, frame count 5030000, time: 159596.43189311028\n",
      "best score of last 100: 945.0, running reward: 328.70 at episode 7606, frame count 5040000, time: 159927.11359405518\n",
      "best score of last 100: 945.0, running reward: 334.10 at episode 7618, frame count 5050000, time: 160256.67057299614\n",
      "best score of last 100: 945.0, running reward: 345.65 at episode 7630, frame count 5060000, time: 160585.0508840084\n",
      "best score of last 100: 945.0, running reward: 344.85 at episode 7643, frame count 5070000, time: 160913.8921740055\n",
      "best score of last 100: 945.0, running reward: 347.35 at episode 7659, frame count 5080000, time: 161244.24263000488\n",
      "best score of last 100: 945.0, running reward: 356.20 at episode 7671, frame count 5090000, time: 161573.78937911987\n",
      "best score of last 100: 945.0, running reward: 361.90 at episode 7685, frame count 5100000, time: 161903.87518310547\n",
      "best score of last 100: 785.0, running reward: 358.25 at episode 7699, frame count 5110000, time: 162234.61498308182\n",
      "best score of last 100: 785.0, running reward: 356.40 at episode 7713, frame count 5120000, time: 162564.9863679409\n",
      "best score of last 100: 685.0, running reward: 346.10 at episode 7726, frame count 5130000, time: 162895.21474003792\n",
      "best score of last 100: 685.0, running reward: 330.95 at episode 7740, frame count 5140000, time: 163224.66570210457\n",
      "best score of last 100: 685.0, running reward: 320.45 at episode 7755, frame count 5150000, time: 163554.47083711624\n",
      "best score of last 100: 730.0, running reward: 326.55 at episode 7769, frame count 5160000, time: 163883.93082904816\n",
      "best score of last 100: 730.0, running reward: 320.10 at episode 7785, frame count 5170000, time: 164213.63821911812\n",
      "best score of last 100: 730.0, running reward: 309.35 at episode 7801, frame count 5180000, time: 164543.78226208687\n",
      "best score of last 100: 760.0, running reward: 313.85 at episode 7817, frame count 5190000, time: 164874.97813105583\n",
      "best score of last 100: 760.0, running reward: 325.20 at episode 7828, frame count 5200000, time: 165206.154184103\n",
      "best score of last 100: 760.0, running reward: 336.95 at episode 7842, frame count 5210000, time: 165536.9345870018\n",
      "best score of last 100: 760.0, running reward: 340.25 at episode 7856, frame count 5220000, time: 165867.5758600235\n",
      "best score of last 100: 760.0, running reward: 335.75 at episode 7871, frame count 5230000, time: 166198.43166208267\n",
      "best score of last 100: 760.0, running reward: 345.15 at episode 7887, frame count 5240000, time: 166530.17394900322\n",
      "best score of last 100: 760.0, running reward: 350.20 at episode 7902, frame count 5250000, time: 166862.745855093\n",
      "best score of last 100: 755.0, running reward: 349.95 at episode 7916, frame count 5260000, time: 167193.4210460186\n",
      "best score of last 100: 755.0, running reward: 336.85 at episode 7930, frame count 5270000, time: 167524.8808810711\n",
      "best score of last 100: 755.0, running reward: 332.25 at episode 7944, frame count 5280000, time: 167856.4230480194\n",
      "best score of last 100: 755.0, running reward: 343.35 at episode 7959, frame count 5290000, time: 168187.9090321064\n",
      "best score of last 100: 755.0, running reward: 329.65 at episode 7976, frame count 5300000, time: 168519.1114590168\n",
      "best score of last 100: 755.0, running reward: 327.80 at episode 7990, frame count 5310000, time: 168850.7526729107\n",
      "best score of last 100: 805.0, running reward: 332.25 at episode 8003, frame count 5320000, time: 169183.375551939\n",
      "best score of last 100: 805.0, running reward: 334.30 at episode 8017, frame count 5330000, time: 169519.15713596344\n",
      "best score of last 100: 805.0, running reward: 326.30 at episode 8034, frame count 5340000, time: 169854.51169896126\n",
      "best score of last 100: 805.0, running reward: 323.65 at episode 8050, frame count 5350000, time: 170188.90386009216\n",
      "best score of last 100: 805.0, running reward: 318.80 at episode 8065, frame count 5360000, time: 170523.67872595787\n",
      "best score of last 100: 805.0, running reward: 333.15 at episode 8078, frame count 5370000, time: 170858.2792711258\n",
      "best score of last 100: 805.0, running reward: 327.35 at episode 8092, frame count 5380000, time: 171193.81887292862\n",
      "best score of last 100: 725.0, running reward: 312.45 at episode 8108, frame count 5390000, time: 171526.53355908394\n",
      "best score of last 100: 725.0, running reward: 310.00 at episode 8125, frame count 5400000, time: 171858.3919301033\n",
      "best score of last 100: 800.0, running reward: 321.60 at episode 8139, frame count 5410000, time: 172189.4889240265\n",
      "best score of last 100: 800.0, running reward: 323.45 at episode 8154, frame count 5420000, time: 172520.74045801163\n",
      "best score of last 100: 800.0, running reward: 335.45 at episode 8169, frame count 5430000, time: 172851.53429698944\n",
      "best score of last 100: 800.0, running reward: 324.10 at episode 8185, frame count 5440000, time: 173182.50751900673\n",
      "best score of last 100: 885.0, running reward: 329.05 at episode 8199, frame count 5450000, time: 173513.4426369667\n",
      "best score of last 100: 920.0, running reward: 344.40 at episode 8212, frame count 5460000, time: 173853.07093310356\n",
      "best score of last 100: 920.0, running reward: 361.45 at episode 8226, frame count 5470000, time: 174185.5888159275\n",
      "best score of last 100: 920.0, running reward: 356.35 at episode 8240, frame count 5480000, time: 174518.59825992584\n",
      "best score of last 100: 920.0, running reward: 357.80 at episode 8255, frame count 5490000, time: 174857.79397511482\n",
      "best score of last 100: 920.0, running reward: 354.95 at episode 8268, frame count 5500000, time: 175201.8185930252\n",
      "best score of last 100: 920.0, running reward: 353.85 at episode 8282, frame count 5510000, time: 175538.42824101448\n",
      "best score of last 100: 920.0, running reward: 351.40 at episode 8296, frame count 5520000, time: 175888.1175069809\n",
      "best score of last 100: 880.0, running reward: 346.60 at episode 8310, frame count 5530000, time: 176235.33103108406\n",
      "best score of last 100: 775.0, running reward: 339.00 at episode 8324, frame count 5540000, time: 176581.0109219551\n",
      "best score of last 100: 775.0, running reward: 354.90 at episode 8337, frame count 5550000, time: 176922.69302797318\n",
      "best score of last 100: 775.0, running reward: 354.75 at episode 8352, frame count 5560000, time: 177278.59569501877\n",
      "best score of last 100: 775.0, running reward: 351.10 at episode 8366, frame count 5570000, time: 177634.23818802834\n",
      "best score of last 100: 775.0, running reward: 350.35 at episode 8379, frame count 5580000, time: 177988.93892407417\n",
      "best score of last 100: 785.0, running reward: 349.00 at episode 8396, frame count 5590000, time: 178334.0308430195\n",
      "best score of last 100: 785.0, running reward: 353.75 at episode 8408, frame count 5600000, time: 178676.06013703346\n",
      "best score of last 100: 785.0, running reward: 341.80 at episode 8424, frame count 5610000, time: 179015.88173794746\n",
      "best score of last 100: 1060.0, running reward: 355.25 at episode 8435, frame count 5620000, time: 179351.03336691856\n",
      "best score of last 100: 1060.0, running reward: 357.60 at episode 8449, frame count 5630000, time: 179684.22743701935\n",
      "best score of last 100: 1060.0, running reward: 352.20 at episode 8463, frame count 5640000, time: 180017.84296894073\n",
      "best score of last 100: 1060.0, running reward: 365.70 at episode 8477, frame count 5650000, time: 180351.12541413307\n",
      "best score of last 100: 1060.0, running reward: 368.10 at episode 8491, frame count 5660000, time: 180683.53281903267\n",
      "best score of last 100: 1060.0, running reward: 375.60 at episode 8504, frame count 5670000, time: 181017.80338692665\n",
      "best score of last 100: 1060.0, running reward: 393.90 at episode 8517, frame count 5680000, time: 181351.97244811058\n",
      "best score of last 100: 810.0, running reward: 383.25 at episode 8531, frame count 5690000, time: 181686.32338500023\n",
      "best score of last 100: 810.0, running reward: 381.65 at episode 8543, frame count 5700000, time: 182021.312967062\n",
      "best score of last 100: 870.0, running reward: 389.05 at episode 8556, frame count 5710000, time: 182355.43677711487\n",
      "best score of last 100: 870.0, running reward: 400.85 at episode 8569, frame count 5720000, time: 182689.7271130085\n",
      "best score of last 100: 885.0, running reward: 395.40 at episode 8585, frame count 5730000, time: 183023.6515159607\n",
      "best score of last 100: 885.0, running reward: 378.15 at episode 8601, frame count 5740000, time: 183357.75395703316\n",
      "best score of last 100: 885.0, running reward: 372.10 at episode 8615, frame count 5750000, time: 183690.42413306236\n",
      "best score of last 100: 885.0, running reward: 361.10 at episode 8631, frame count 5760000, time: 184022.6142950058\n",
      "best score of last 100: 885.0, running reward: 361.05 at episode 8644, frame count 5770000, time: 184357.82885694504\n",
      "best score of last 100: 885.0, running reward: 351.20 at episode 8658, frame count 5780000, time: 184689.51903295517\n",
      "best score of last 100: 845.0, running reward: 337.40 at episode 8673, frame count 5790000, time: 185021.37167406082\n",
      "best score of last 100: 845.0, running reward: 339.35 at episode 8689, frame count 5800000, time: 185352.420222044\n",
      "best score of last 100: 845.0, running reward: 346.75 at episode 8703, frame count 5810000, time: 185684.3031849861\n",
      "best score of last 100: 845.0, running reward: 342.80 at episode 8718, frame count 5820000, time: 186015.50795793533\n",
      "best score of last 100: 845.0, running reward: 348.80 at episode 8733, frame count 5830000, time: 186346.02025294304\n",
      "best score of last 100: 665.0, running reward: 333.10 at episode 8748, frame count 5840000, time: 186676.47020292282\n",
      "best score of last 100: 745.0, running reward: 321.10 at episode 8764, frame count 5850000, time: 187007.0277070999\n",
      "best score of last 100: 760.0, running reward: 331.20 at episode 8777, frame count 5860000, time: 187338.42674613\n",
      "best score of last 100: 760.0, running reward: 339.65 at episode 8789, frame count 5870000, time: 187669.6408109665\n",
      "best score of last 100: 760.0, running reward: 342.30 at episode 8802, frame count 5880000, time: 188001.38003897667\n",
      "best score of last 100: 760.0, running reward: 335.40 at episode 8818, frame count 5890000, time: 188334.93075299263\n",
      "best score of last 100: 760.0, running reward: 343.75 at episode 8831, frame count 5900000, time: 188667.9789481163\n",
      "best score of last 100: 760.0, running reward: 362.75 at episode 8844, frame count 5910000, time: 189002.61917996407\n",
      "best score of last 100: 790.0, running reward: 379.40 at episode 8858, frame count 5920000, time: 189334.89188313484\n",
      "best score of last 100: 790.0, running reward: 363.75 at episode 8874, frame count 5930000, time: 189668.45727610588\n",
      "best score of last 100: 790.0, running reward: 367.05 at episode 8886, frame count 5940000, time: 190001.9760389328\n",
      "best score of last 100: 790.0, running reward: 358.80 at episode 8900, frame count 5950000, time: 190336.1061310768\n",
      "best score of last 100: 790.0, running reward: 362.00 at episode 8913, frame count 5960000, time: 190669.71003508568\n",
      "best score of last 100: 790.0, running reward: 358.55 at episode 8928, frame count 5970000, time: 191003.42132401466\n",
      "best score of last 100: 790.0, running reward: 349.70 at episode 8941, frame count 5980000, time: 191337.74962306023\n",
      "best score of last 100: 715.0, running reward: 334.85 at episode 8955, frame count 5990000, time: 191672.13286614418\n",
      "best score of last 100: 630.0, running reward: 337.15 at episode 8969, frame count 6000000, time: 192006.0892329216\n",
      "best score of last 100: 625.0, running reward: 338.05 at episode 8984, frame count 6010000, time: 192339.074780941\n",
      "best score of last 100: 650.0, running reward: 346.55 at episode 8998, frame count 6020000, time: 192671.99697995186\n",
      "best score of last 100: 720.0, running reward: 345.80 at episode 9013, frame count 6030000, time: 193007.42853999138\n",
      "best score of last 100: 720.0, running reward: 345.75 at episode 9029, frame count 6040000, time: 193341.23952913284\n",
      "best score of last 100: 720.0, running reward: 345.25 at episode 9042, frame count 6050000, time: 193674.5026819706\n",
      "best score of last 100: 720.0, running reward: 349.80 at episode 9056, frame count 6060000, time: 194008.70037412643\n",
      "best score of last 100: 745.0, running reward: 346.50 at episode 9072, frame count 6070000, time: 194343.41251206398\n",
      "best score of last 100: 745.0, running reward: 332.40 at episode 9088, frame count 6080000, time: 194678.34416508675\n",
      "best score of last 100: 745.0, running reward: 326.85 at episode 9102, frame count 6090000, time: 195013.5254700184\n",
      "best score of last 100: 745.0, running reward: 330.05 at episode 9115, frame count 6100000, time: 195348.68984508514\n",
      "best score of last 100: 745.0, running reward: 339.00 at episode 9129, frame count 6110000, time: 195684.17210912704\n",
      "best score of last 100: 745.0, running reward: 337.45 at episode 9145, frame count 6120000, time: 196019.01479506493\n",
      "best score of last 100: 745.0, running reward: 334.50 at episode 9162, frame count 6130000, time: 196355.16688394547\n",
      "best score of last 100: 755.0, running reward: 328.35 at episode 9178, frame count 6140000, time: 196690.74148511887\n",
      "best score of last 100: 755.0, running reward: 342.45 at episode 9191, frame count 6150000, time: 197026.1057600975\n",
      "best score of last 100: 755.0, running reward: 330.45 at episode 9208, frame count 6160000, time: 197362.3561220169\n",
      "best score of last 100: 755.0, running reward: 329.95 at episode 9222, frame count 6170000, time: 197697.60343503952\n",
      "best score of last 100: 755.0, running reward: 319.35 at episode 9238, frame count 6180000, time: 198032.84898209572\n",
      "best score of last 100: 755.0, running reward: 322.80 at episode 9253, frame count 6190000, time: 198367.16113114357\n",
      "best score of last 100: 805.0, running reward: 339.00 at episode 9267, frame count 6200000, time: 198701.21189808846\n",
      "best score of last 100: 805.0, running reward: 337.50 at episode 9282, frame count 6210000, time: 199035.38376402855\n",
      "best score of last 100: 1015.0, running reward: 341.70 at episode 9295, frame count 6220000, time: 199368.59431695938\n",
      "best score of last 100: 1015.0, running reward: 342.75 at episode 9310, frame count 6230000, time: 199701.80928993225\n",
      "best score of last 100: 1015.0, running reward: 347.65 at episode 9324, frame count 6240000, time: 200034.4771440029\n",
      "best score of last 100: 1015.0, running reward: 359.95 at episode 9337, frame count 6250000, time: 200367.5595281124\n",
      "best score of last 100: 1015.0, running reward: 364.20 at episode 9352, frame count 6260000, time: 200700.55298900604\n",
      "best score of last 100: 1015.0, running reward: 360.75 at episode 9364, frame count 6270000, time: 201033.87345314026\n",
      "best score of last 100: 1015.0, running reward: 365.60 at episode 9377, frame count 6280000, time: 201367.10269498825\n",
      "best score of last 100: 1015.0, running reward: 371.50 at episode 9392, frame count 6290000, time: 201700.52332091331\n",
      "best score of last 100: 925.0, running reward: 363.65 at episode 9409, frame count 6300000, time: 202036.3034760952\n",
      "best score of last 100: 925.0, running reward: 372.25 at episode 9422, frame count 6310000, time: 202370.14180493355\n",
      "best score of last 100: 885.0, running reward: 358.00 at episode 9437, frame count 6320000, time: 202705.4789030552\n",
      "best score of last 100: 840.0, running reward: 359.15 at episode 9452, frame count 6330000, time: 203041.3111331463\n",
      "best score of last 100: 840.0, running reward: 349.20 at episode 9465, frame count 6340000, time: 203376.47298502922\n",
      "best score of last 100: 885.0, running reward: 349.65 at episode 9480, frame count 6350000, time: 203712.2552421093\n",
      "best score of last 100: 885.0, running reward: 354.25 at episode 9494, frame count 6360000, time: 204048.50296401978\n",
      "best score of last 100: 885.0, running reward: 361.60 at episode 9507, frame count 6370000, time: 204385.40785503387\n",
      "best score of last 100: 885.0, running reward: 342.15 at episode 9523, frame count 6380000, time: 204721.5422050953\n",
      "best score of last 100: 885.0, running reward: 342.90 at episode 9538, frame count 6390000, time: 205057.4387640953\n",
      "best score of last 100: 885.0, running reward: 342.90 at episode 9552, frame count 6400000, time: 205393.59056711197\n",
      "best score of last 100: 885.0, running reward: 349.70 at episode 9565, frame count 6410000, time: 205729.7953400612\n",
      "best score of last 100: 785.0, running reward: 346.25 at episode 9580, frame count 6420000, time: 206065.31086301804\n",
      "best score of last 100: 935.0, running reward: 347.15 at episode 9595, frame count 6430000, time: 206401.69707107544\n",
      "best score of last 100: 935.0, running reward: 336.35 at episode 9612, frame count 6440000, time: 206739.62712812424\n",
      "best score of last 100: 935.0, running reward: 347.70 at episode 9627, frame count 6450000, time: 207076.68472599983\n",
      "best score of last 100: 935.0, running reward: 349.70 at episode 9641, frame count 6460000, time: 207413.18803811073\n",
      "best score of last 100: 935.0, running reward: 350.75 at episode 9655, frame count 6470000, time: 207749.86538290977\n",
      "best score of last 100: 935.0, running reward: 353.65 at episode 9670, frame count 6480000, time: 208087.7632329464\n",
      "best score of last 100: 935.0, running reward: 350.55 at episode 9684, frame count 6490000, time: 208426.0601799488\n",
      "best score of last 100: 800.0, running reward: 353.25 at episode 9697, frame count 6500000, time: 208764.349517107\n",
      "best score of last 100: 800.0, running reward: 358.35 at episode 9714, frame count 6510000, time: 209101.20306706429\n",
      "best score of last 100: 805.0, running reward: 346.80 at episode 9729, frame count 6520000, time: 209437.15039491653\n",
      "best score of last 100: 805.0, running reward: 329.70 at episode 9745, frame count 6530000, time: 209773.8222310543\n",
      "best score of last 100: 805.0, running reward: 320.75 at episode 9758, frame count 6540000, time: 210110.39588809013\n",
      "best score of last 100: 805.0, running reward: 319.20 at episode 9772, frame count 6550000, time: 210446.63654899597\n",
      "best score of last 100: 805.0, running reward: 326.00 at episode 9786, frame count 6560000, time: 210783.7508509159\n",
      "best score of last 100: 805.0, running reward: 325.60 at episode 9800, frame count 6570000, time: 211119.78461694717\n",
      "best score of last 100: 805.0, running reward: 336.80 at episode 9813, frame count 6580000, time: 211460.3337841034\n",
      "best score of last 100: 745.0, running reward: 343.65 at episode 9827, frame count 6590000, time: 211799.2429909706\n",
      "best score of last 100: 745.0, running reward: 352.40 at episode 9841, frame count 6600000, time: 212137.87952303886\n",
      "best score of last 100: 745.0, running reward: 352.05 at episode 9857, frame count 6610000, time: 212476.64688110352\n",
      "best score of last 100: 745.0, running reward: 347.75 at episode 9871, frame count 6620000, time: 212814.8135509491\n",
      "best score of last 100: 745.0, running reward: 349.50 at episode 9885, frame count 6630000, time: 213153.66525292397\n",
      "best score of last 100: 770.0, running reward: 349.80 at episode 9899, frame count 6640000, time: 213492.63308906555\n",
      "best score of last 100: 770.0, running reward: 345.35 at episode 9912, frame count 6650000, time: 213840.72665691376\n",
      "best score of last 100: 770.0, running reward: 348.15 at episode 9925, frame count 6660000, time: 214200.44887900352\n",
      "best score of last 100: 770.0, running reward: 351.70 at episode 9939, frame count 6670000, time: 214564.53658604622\n",
      "best score of last 100: 770.0, running reward: 360.20 at episode 9953, frame count 6680000, time: 214931.45297002792\n",
      "best score of last 100: 770.0, running reward: 372.85 at episode 9965, frame count 6690000, time: 215304.83341908455\n",
      "best score of last 100: 770.0, running reward: 383.55 at episode 9975, frame count 6700000, time: 215647.86179804802\n",
      "best score of last 100: 780.0, running reward: 389.35 at episode 9986, frame count 6710000, time: 215993.85493206978\n",
      "best score of last 100: 780.0, running reward: 393.20 at episode 9999, frame count 6720000, time: 216338.26668691635\n",
      "best score of last 100: 780.0, running reward: 394.10 at episode 10012, frame count 6730000, time: 216684.9189031124\n",
      "best score of last 100: 780.0, running reward: 391.50 at episode 10025, frame count 6740000, time: 217033.2336730957\n",
      "best score of last 100: 780.0, running reward: 399.75 at episode 10037, frame count 6750000, time: 217375.97639608383\n",
      "best score of last 100: 780.0, running reward: 406.80 at episode 10050, frame count 6760000, time: 217721.26514101028\n",
      "best score of last 100: 780.0, running reward: 397.80 at episode 10064, frame count 6770000, time: 218066.0791990757\n",
      "best score of last 100: 780.0, running reward: 383.50 at episode 10079, frame count 6780000, time: 218410.82863593102\n",
      "best score of last 100: 735.0, running reward: 363.30 at episode 10096, frame count 6790000, time: 218754.8502459526\n",
      "best score of last 100: 675.0, running reward: 353.80 at episode 10111, frame count 6800000, time: 219101.4705300331\n",
      "best score of last 100: 675.0, running reward: 349.05 at episode 10124, frame count 6810000, time: 219447.0682311058\n",
      "best score of last 100: 985.0, running reward: 349.05 at episode 10137, frame count 6820000, time: 219792.99481797218\n",
      "best score of last 100: 985.0, running reward: 346.40 at episode 10150, frame count 6830000, time: 220138.78647708893\n",
      "best score of last 100: 985.0, running reward: 346.25 at episode 10164, frame count 6840000, time: 220484.39035391808\n",
      "best score of last 100: 985.0, running reward: 358.30 at episode 10177, frame count 6850000, time: 220829.33776903152\n",
      "best score of last 100: 985.0, running reward: 375.40 at episode 10190, frame count 6860000, time: 221174.44383096695\n",
      "best score of last 100: 985.0, running reward: 386.75 at episode 10201, frame count 6870000, time: 221520.0063841343\n",
      "best score of last 100: 985.0, running reward: 400.65 at episode 10213, frame count 6880000, time: 221864.80226898193\n",
      "best score of last 100: 985.0, running reward: 399.45 at episode 10227, frame count 6890000, time: 222210.5428700447\n",
      "best score of last 100: 810.0, running reward: 398.60 at episode 10240, frame count 6900000, time: 222555.34977006912\n",
      "best score of last 100: 955.0, running reward: 404.10 at episode 10253, frame count 6910000, time: 222900.9089269638\n",
      "best score of last 100: 955.0, running reward: 405.75 at episode 10266, frame count 6920000, time: 223245.16006708145\n",
      "best score of last 100: 955.0, running reward: 407.70 at episode 10278, frame count 6930000, time: 223590.45662999153\n",
      "best score of last 100: 955.0, running reward: 416.15 at episode 10290, frame count 6940000, time: 223935.88847613335\n",
      "best score of last 100: 955.0, running reward: 393.25 at episode 10306, frame count 6950000, time: 224290.71519804\n",
      "best score of last 100: 955.0, running reward: 394.20 at episode 10319, frame count 6960000, time: 224636.21005296707\n",
      "best score of last 100: 955.0, running reward: 392.10 at episode 10334, frame count 6970000, time: 224980.8106701374\n",
      "best score of last 100: 955.0, running reward: 390.35 at episode 10348, frame count 6980000, time: 225325.93119597435\n",
      "best score of last 100: 945.0, running reward: 375.25 at episode 10363, frame count 6990000, time: 225671.33721613884\n",
      "best score of last 100: 830.0, running reward: 366.05 at episode 10376, frame count 7000000, time: 226016.46624803543\n",
      "best score of last 100: 830.0, running reward: 343.85 at episode 10393, frame count 7010000, time: 226362.28131508827\n",
      "best score of last 100: 800.0, running reward: 339.80 at episode 10408, frame count 7020000, time: 226707.9013581276\n",
      "best score of last 100: 810.0, running reward: 340.05 at episode 10420, frame count 7030000, time: 227054.02740693092\n",
      "best score of last 100: 810.0, running reward: 358.55 at episode 10433, frame count 7040000, time: 227400.41418504715\n",
      "best score of last 100: 850.0, running reward: 359.65 at episode 10447, frame count 7050000, time: 227746.77510499954\n",
      "best score of last 100: 850.0, running reward: 363.65 at episode 10461, frame count 7060000, time: 228092.6320860386\n",
      "best score of last 100: 850.0, running reward: 366.55 at episode 10475, frame count 7070000, time: 228438.19991707802\n",
      "best score of last 100: 850.0, running reward: 372.05 at episode 10488, frame count 7080000, time: 228785.40770196915\n",
      "best score of last 100: 850.0, running reward: 378.95 at episode 10503, frame count 7090000, time: 229130.36256313324\n",
      "best score of last 100: 850.0, running reward: 375.75 at episode 10518, frame count 7100000, time: 229475.562251091\n",
      "best score of last 100: 850.0, running reward: 356.25 at episode 10531, frame count 7110000, time: 229819.14487409592\n",
      "best score of last 100: 780.0, running reward: 349.10 at episode 10545, frame count 7120000, time: 230162.4387319088\n",
      "best score of last 100: 780.0, running reward: 355.30 at episode 10557, frame count 7130000, time: 230506.9254131317\n",
      "best score of last 100: 780.0, running reward: 349.75 at episode 10571, frame count 7140000, time: 230851.62359905243\n",
      "best score of last 100: 770.0, running reward: 355.40 at episode 10584, frame count 7150000, time: 231197.4762980938\n",
      "best score of last 100: 770.0, running reward: 356.10 at episode 10598, frame count 7160000, time: 231543.66559004784\n",
      "best score of last 100: 810.0, running reward: 369.40 at episode 10610, frame count 7170000, time: 231893.17681097984\n",
      "best score of last 100: 810.0, running reward: 371.05 at episode 10624, frame count 7180000, time: 232240.31000113487\n",
      "best score of last 100: 810.0, running reward: 360.25 at episode 10641, frame count 7190000, time: 232587.26696109772\n",
      "best score of last 100: 810.0, running reward: 361.95 at episode 10653, frame count 7200000, time: 232935.4832971096\n",
      "best score of last 100: 810.0, running reward: 359.60 at episode 10666, frame count 7210000, time: 233283.38233208656\n",
      "best score of last 100: 810.0, running reward: 360.10 at episode 10678, frame count 7220000, time: 233631.90109801292\n",
      "best score of last 100: 810.0, running reward: 361.90 at episode 10692, frame count 7230000, time: 233980.66559791565\n",
      "best score of last 100: 710.0, running reward: 348.80 at episode 10706, frame count 7240000, time: 234329.00194311142\n",
      "best score of last 100: 710.0, running reward: 356.65 at episode 10719, frame count 7250000, time: 234675.89613103867\n",
      "best score of last 100: 680.0, running reward: 361.85 at episode 10733, frame count 7260000, time: 235023.25136899948\n",
      "best score of last 100: 800.0, running reward: 374.15 at episode 10746, frame count 7270000, time: 235371.30039906502\n",
      "best score of last 100: 800.0, running reward: 366.15 at episode 10760, frame count 7280000, time: 235719.53754591942\n",
      "best score of last 100: 800.0, running reward: 353.60 at episode 10776, frame count 7290000, time: 236066.86669707298\n",
      "best score of last 100: 800.0, running reward: 351.45 at episode 10790, frame count 7300000, time: 236415.0124759674\n",
      "best score of last 100: 800.0, running reward: 351.55 at episode 10804, frame count 7310000, time: 236764.21352910995\n",
      "best score of last 100: 800.0, running reward: 354.00 at episode 10818, frame count 7320000, time: 237109.97143292427\n",
      "best score of last 100: 800.0, running reward: 348.55 at episode 10834, frame count 7330000, time: 237455.45069408417\n",
      "best score of last 100: 745.0, running reward: 344.95 at episode 10847, frame count 7340000, time: 237800.60602593422\n",
      "best score of last 100: 745.0, running reward: 350.95 at episode 10861, frame count 7350000, time: 238145.77105212212\n",
      "best score of last 100: 745.0, running reward: 344.90 at episode 10876, frame count 7360000, time: 238490.99527692795\n",
      "best score of last 100: 905.0, running reward: 360.90 at episode 10887, frame count 7370000, time: 238836.72404313087\n",
      "best score of last 100: 905.0, running reward: 368.60 at episode 10899, frame count 7380000, time: 239184.2611670494\n",
      "best score of last 100: 905.0, running reward: 364.35 at episode 10913, frame count 7390000, time: 239539.28312206268\n",
      "best score of last 100: 905.0, running reward: 360.20 at episode 10927, frame count 7400000, time: 239888.89453101158\n",
      "best score of last 100: 905.0, running reward: 348.30 at episode 10943, frame count 7410000, time: 240238.053483963\n",
      "best score of last 100: 905.0, running reward: 342.25 at episode 10960, frame count 7420000, time: 240588.11309194565\n",
      "best score of last 100: 905.0, running reward: 341.35 at episode 10973, frame count 7430000, time: 240937.90907502174\n",
      "best score of last 100: 690.0, running reward: 332.05 at episode 10988, frame count 7440000, time: 241287.76054906845\n",
      "best score of last 100: 690.0, running reward: 315.40 at episode 11002, frame count 7450000, time: 241637.08802199364\n",
      "best score of last 100: 685.0, running reward: 308.65 at episode 11016, frame count 7460000, time: 241986.5803041458\n",
      "best score of last 100: 685.0, running reward: 308.10 at episode 11031, frame count 7470000, time: 242336.36569404602\n",
      "best score of last 100: 685.0, running reward: 308.95 at episode 11047, frame count 7480000, time: 242686.2366719246\n",
      "best score of last 100: 685.0, running reward: 305.10 at episode 11064, frame count 7490000, time: 243035.5787370205\n",
      "best score of last 100: 685.0, running reward: 303.65 at episode 11078, frame count 7500000, time: 243384.90460395813\n",
      "best score of last 100: 665.0, running reward: 297.10 at episode 11093, frame count 7510000, time: 243740.4430680275\n",
      "best score of last 100: 665.0, running reward: 303.65 at episode 11107, frame count 7520000, time: 244088.83336997032\n",
      "best score of last 100: 555.0, running reward: 297.00 at episode 11123, frame count 7530000, time: 244433.26047301292\n",
      "best score of last 100: 660.0, running reward: 296.05 at episode 11136, frame count 7540000, time: 244777.82795405388\n",
      "best score of last 100: 660.0, running reward: 305.30 at episode 11152, frame count 7550000, time: 245144.06321811676\n",
      "best score of last 100: 660.0, running reward: 312.90 at episode 11165, frame count 7560000, time: 245529.94481492043\n",
      "best score of last 100: 695.0, running reward: 316.00 at episode 11178, frame count 7570000, time: 245889.87516498566\n",
      "best score of last 100: 730.0, running reward: 320.75 at episode 11192, frame count 7580000, time: 246248.1469631195\n",
      "best score of last 100: 730.0, running reward: 330.95 at episode 11207, frame count 7590000, time: 246605.82881212234\n",
      "best score of last 100: 730.0, running reward: 338.75 at episode 11221, frame count 7600000, time: 246963.8580751419\n",
      "best score of last 100: 730.0, running reward: 348.95 at episode 11233, frame count 7610000, time: 247319.02831602097\n",
      "best score of last 100: 730.0, running reward: 357.55 at episode 11244, frame count 7620000, time: 247672.93853497505\n",
      "best score of last 100: 750.0, running reward: 361.10 at episode 11260, frame count 7630000, time: 248027.56237006187\n",
      "best score of last 100: 750.0, running reward: 362.05 at episode 11274, frame count 7640000, time: 248379.94497799873\n",
      "best score of last 100: 750.0, running reward: 356.60 at episode 11289, frame count 7650000, time: 248733.52387309074\n",
      "best score of last 100: 750.0, running reward: 351.85 at episode 11304, frame count 7660000, time: 249087.5999469757\n",
      "best score of last 100: 875.0, running reward: 363.15 at episode 11318, frame count 7670000, time: 249448.23749494553\n",
      "best score of last 100: 875.0, running reward: 352.80 at episode 11333, frame count 7680000, time: 249832.08728694916\n",
      "best score of last 100: 875.0, running reward: 352.80 at episode 11346, frame count 7690000, time: 250197.10193896294\n",
      "best score of last 100: 875.0, running reward: 351.70 at episode 11361, frame count 7700000, time: 250561.73373293877\n",
      "best score of last 100: 875.0, running reward: 342.00 at episode 11377, frame count 7710000, time: 250920.35467410088\n",
      "best score of last 100: 875.0, running reward: 342.90 at episode 11392, frame count 7720000, time: 251270.36287212372\n",
      "best score of last 100: 875.0, running reward: 335.10 at episode 11406, frame count 7730000, time: 251622.23456001282\n",
      "best score of last 100: 815.0, running reward: 327.90 at episode 11420, frame count 7740000, time: 251974.64209604263\n",
      "best score of last 100: 815.0, running reward: 321.25 at episode 11435, frame count 7750000, time: 252327.03942012787\n",
      "best score of last 100: 810.0, running reward: 309.55 at episode 11449, frame count 7760000, time: 252684.8589580059\n",
      "best score of last 100: 810.0, running reward: 321.55 at episode 11462, frame count 7770000, time: 253054.96607208252\n",
      "best score of last 100: 810.0, running reward: 333.80 at episode 11475, frame count 7780000, time: 253445.97431707382\n",
      "best score of last 100: 810.0, running reward: 338.95 at episode 11490, frame count 7790000, time: 253825.5459010601\n",
      "best score of last 100: 810.0, running reward: 356.30 at episode 11503, frame count 7800000, time: 254209.613312006\n",
      "best score of last 100: 760.0, running reward: 350.15 at episode 11518, frame count 7810000, time: 254581.39366912842\n",
      "best score of last 100: 800.0, running reward: 373.65 at episode 11530, frame count 7820000, time: 254950.22191905975\n",
      "best score of last 100: 800.0, running reward: 389.05 at episode 11543, frame count 7830000, time: 255311.18340301514\n",
      "best score of last 100: 800.0, running reward: 393.85 at episode 11556, frame count 7840000, time: 255683.89025306702\n",
      "best score of last 100: 800.0, running reward: 379.50 at episode 11572, frame count 7850000, time: 256047.05770611763\n",
      "best score of last 100: 980.0, running reward: 379.05 at episode 11585, frame count 7860000, time: 256422.625177145\n",
      "best score of last 100: 980.0, running reward: 381.60 at episode 11600, frame count 7870000, time: 256785.85344696045\n",
      "best score of last 100: 980.0, running reward: 377.95 at episode 11612, frame count 7880000, time: 257166.62222909927\n",
      "best score of last 100: 980.0, running reward: 395.30 at episode 11622, frame count 7890000, time: 257535.93175196648\n",
      "best score of last 100: 980.0, running reward: 374.60 at episode 11637, frame count 7900000, time: 257912.76590394974\n",
      "best score of last 100: 980.0, running reward: 375.50 at episode 11649, frame count 7910000, time: 258293.08625006676\n",
      "best score of last 100: 980.0, running reward: 381.70 at episode 11661, frame count 7920000, time: 258661.55895400047\n",
      "best score of last 100: 910.0, running reward: 376.80 at episode 11676, frame count 7930000, time: 259036.12131500244\n",
      "best score of last 100: 910.0, running reward: 372.75 at episode 11691, frame count 7940000, time: 259409.2022099495\n",
      "best score of last 100: 910.0, running reward: 350.85 at episode 11707, frame count 7950000, time: 259782.1042649746\n",
      "best score of last 100: 790.0, running reward: 338.60 at episode 11721, frame count 7960000, time: 260168.45092391968\n",
      "best score of last 100: 840.0, running reward: 354.20 at episode 11733, frame count 7970000, time: 260531.14427900314\n",
      "best score of last 100: 840.0, running reward: 351.10 at episode 11747, frame count 7980000, time: 260896.71419596672\n",
      "best score of last 100: 840.0, running reward: 339.80 at episode 11760, frame count 7990000, time: 261295.05248999596\n",
      "best score of last 100: 840.0, running reward: 349.60 at episode 11773, frame count 8000000, time: 261679.74846291542\n",
      "best score of last 100: 840.0, running reward: 351.75 at episode 11787, frame count 8010000, time: 262065.61214613914\n",
      "best score of last 100: 870.0, running reward: 363.25 at episode 11800, frame count 8020000, time: 262442.5059020519\n",
      "best score of last 100: 870.0, running reward: 378.45 at episode 11814, frame count 8030000, time: 262811.2919960022\n",
      "best score of last 100: 870.0, running reward: 360.95 at episode 11828, frame count 8040000, time: 263180.44789099693\n",
      "best score of last 100: 925.0, running reward: 350.30 at episode 11843, frame count 8050000, time: 263539.8227510452\n",
      "best score of last 100: 925.0, running reward: 349.85 at episode 11856, frame count 8060000, time: 263921.06768608093\n",
      "best score of last 100: 925.0, running reward: 353.30 at episode 11870, frame count 8070000, time: 264308.36248898506\n",
      "best score of last 100: 925.0, running reward: 333.40 at episode 11885, frame count 8080000, time: 264746.0679490566\n",
      "best score of last 100: 925.0, running reward: 333.20 at episode 11897, frame count 8090000, time: 265206.8742120266\n",
      "best score of last 100: 925.0, running reward: 340.95 at episode 11910, frame count 8100000, time: 265680.9488809109\n",
      "best score of last 100: 925.0, running reward: 341.75 at episode 11923, frame count 8110000, time: 266155.85785007477\n",
      "best score of last 100: 925.0, running reward: 355.30 at episode 11935, frame count 8120000, time: 266629.6866350174\n",
      "best score of last 100: 750.0, running reward: 353.15 at episode 11950, frame count 8130000, time: 267101.85329914093\n",
      "best score of last 100: 750.0, running reward: 357.30 at episode 11963, frame count 8140000, time: 267580.5981750488\n",
      "best score of last 100: 750.0, running reward: 369.20 at episode 11974, frame count 8150000, time: 268087.9216029644\n",
      "best score of last 100: 1175.0, running reward: 383.60 at episode 11989, frame count 8160000, time: 268603.0187239647\n",
      "best score of last 100: 1175.0, running reward: 374.80 at episode 12002, frame count 8170000, time: 269115.81996512413\n",
      "best score of last 100: 1175.0, running reward: 372.05 at episode 12016, frame count 8180000, time: 269592.1085650921\n",
      "best score of last 100: 1175.0, running reward: 353.60 at episode 12032, frame count 8190000, time: 270056.3968400955\n",
      "best score of last 100: 1175.0, running reward: 362.40 at episode 12044, frame count 8200000, time: 270413.1175839901\n",
      "best score of last 100: 1175.0, running reward: 358.45 at episode 12059, frame count 8210000, time: 270770.33526706696\n",
      "best score of last 100: 1175.0, running reward: 346.60 at episode 12072, frame count 8220000, time: 271121.794809103\n",
      "best score of last 100: 910.0, running reward: 346.00 at episode 12084, frame count 8230000, time: 271473.5035350323\n",
      "best score of last 100: 910.0, running reward: 351.80 at episode 12098, frame count 8240000, time: 271825.0635061264\n",
      "best score of last 100: 840.0, running reward: 362.15 at episode 12110, frame count 8250000, time: 272177.6016430855\n",
      "best score of last 100: 840.0, running reward: 369.80 at episode 12124, frame count 8260000, time: 272530.70399594307\n",
      "best score of last 100: 840.0, running reward: 362.50 at episode 12139, frame count 8270000, time: 272885.80349612236\n",
      "best score of last 100: 840.0, running reward: 359.85 at episode 12153, frame count 8280000, time: 273241.6434509754\n",
      "best score of last 100: 840.0, running reward: 359.80 at episode 12166, frame count 8290000, time: 273596.68972706795\n",
      "best score of last 100: 840.0, running reward: 356.50 at episode 12179, frame count 8300000, time: 273949.6207239628\n",
      "best score of last 100: 840.0, running reward: 352.90 at episode 12193, frame count 8310000, time: 274301.6058320999\n",
      "best score of last 100: 840.0, running reward: 351.00 at episode 12207, frame count 8320000, time: 274657.7053780556\n",
      "best score of last 100: 700.0, running reward: 354.90 at episode 12219, frame count 8330000, time: 275015.1570959091\n",
      "best score of last 100: 700.0, running reward: 357.80 at episode 12233, frame count 8340000, time: 275379.71706199646\n",
      "best score of last 100: 700.0, running reward: 362.05 at episode 12247, frame count 8350000, time: 275733.1125230789\n",
      "best score of last 100: 735.0, running reward: 368.80 at episode 12260, frame count 8360000, time: 276089.0904560089\n",
      "best score of last 100: 735.0, running reward: 370.95 at episode 12272, frame count 8370000, time: 276447.7435951233\n",
      "best score of last 100: 735.0, running reward: 387.70 at episode 12285, frame count 8380000, time: 276800.92417907715\n",
      "best score of last 100: 870.0, running reward: 389.15 at episode 12297, frame count 8390000, time: 277154.19230914116\n",
      "best score of last 100: 870.0, running reward: 392.00 at episode 12310, frame count 8400000, time: 277510.7754421234\n",
      "best score of last 100: 870.0, running reward: 389.60 at episode 12322, frame count 8410000, time: 277866.34617209435\n",
      "best score of last 100: 870.0, running reward: 389.85 at episode 12337, frame count 8420000, time: 278222.71466612816\n",
      "best score of last 100: 870.0, running reward: 396.05 at episode 12350, frame count 8430000, time: 278576.0148830414\n",
      "best score of last 100: 870.0, running reward: 386.20 at episode 12366, frame count 8440000, time: 278928.7418000698\n",
      "best score of last 100: 870.0, running reward: 377.65 at episode 12380, frame count 8450000, time: 279280.46452093124\n",
      "best score of last 100: 810.0, running reward: 370.70 at episode 12394, frame count 8460000, time: 279637.5507159233\n",
      "best score of last 100: 810.0, running reward: 368.05 at episode 12408, frame count 8470000, time: 279992.99683713913\n",
      "best score of last 100: 805.0, running reward: 356.85 at episode 12422, frame count 8480000, time: 280347.6144580841\n",
      "best score of last 100: 800.0, running reward: 358.45 at episode 12436, frame count 8490000, time: 280701.9788250923\n",
      "best score of last 100: 830.0, running reward: 356.50 at episode 12449, frame count 8500000, time: 281055.25503492355\n",
      "best score of last 100: 830.0, running reward: 359.00 at episode 12463, frame count 8510000, time: 281408.84395098686\n",
      "best score of last 100: 830.0, running reward: 359.40 at episode 12479, frame count 8520000, time: 281762.74522805214\n",
      "best score of last 100: 830.0, running reward: 357.25 at episode 12491, frame count 8530000, time: 282115.9017589092\n",
      "best score of last 100: 830.0, running reward: 359.20 at episode 12505, frame count 8540000, time: 282474.2703781128\n",
      "best score of last 100: 830.0, running reward: 352.65 at episode 12520, frame count 8550000, time: 282827.48530101776\n",
      "best score of last 100: 830.0, running reward: 368.20 at episode 12532, frame count 8560000, time: 283185.3673839569\n",
      "best score of last 100: 830.0, running reward: 367.50 at episode 12545, frame count 8570000, time: 283540.7096309662\n",
      "best score of last 100: 815.0, running reward: 361.80 at episode 12561, frame count 8580000, time: 283898.03445506096\n",
      "best score of last 100: 835.0, running reward: 359.30 at episode 12577, frame count 8590000, time: 284256.15132403374\n",
      "best score of last 100: 835.0, running reward: 366.70 at episode 12590, frame count 8600000, time: 284611.4533829689\n",
      "best score of last 100: 835.0, running reward: 357.65 at episode 12605, frame count 8610000, time: 284965.9901189804\n",
      "best score of last 100: 835.0, running reward: 360.15 at episode 12620, frame count 8620000, time: 285320.85135412216\n",
      "best score of last 100: 835.0, running reward: 344.30 at episode 12635, frame count 8630000, time: 285672.4448559284\n",
      "best score of last 100: 835.0, running reward: 341.65 at episode 12650, frame count 8640000, time: 286030.3465731144\n",
      "best score of last 100: 630.0, running reward: 321.90 at episode 12666, frame count 8650000, time: 286390.73403191566\n",
      "best score of last 100: 665.0, running reward: 325.80 at episode 12681, frame count 8660000, time: 286746.60411190987\n",
      "best score of last 100: 665.0, running reward: 324.85 at episode 12695, frame count 8670000, time: 287098.5119099617\n",
      "best score of last 100: 695.0, running reward: 330.25 at episode 12707, frame count 8680000, time: 287452.93914699554\n",
      "best score of last 100: 695.0, running reward: 328.70 at episode 12721, frame count 8690000, time: 287809.19582414627\n",
      "best score of last 100: 695.0, running reward: 325.05 at episode 12736, frame count 8700000, time: 288166.0043079853\n",
      "best score of last 100: 875.0, running reward: 342.95 at episode 12750, frame count 8710000, time: 288518.2008960247\n",
      "best score of last 100: 905.0, running reward: 350.40 at episode 12763, frame count 8720000, time: 288871.7027001381\n",
      "best score of last 100: 905.0, running reward: 362.60 at episode 12777, frame count 8730000, time: 289225.7630701065\n",
      "best score of last 100: 905.0, running reward: 352.25 at episode 12792, frame count 8740000, time: 289578.6395840645\n",
      "best score of last 100: 905.0, running reward: 340.35 at episode 12808, frame count 8750000, time: 289934.61339592934\n",
      "best score of last 100: 905.0, running reward: 337.20 at episode 12822, frame count 8760000, time: 290295.4453380108\n",
      "best score of last 100: 905.0, running reward: 346.10 at episode 12836, frame count 8770000, time: 290652.76742196083\n",
      "best score of last 100: 910.0, running reward: 337.65 at episode 12852, frame count 8780000, time: 291016.26493906975\n",
      "best score of last 100: 910.0, running reward: 340.40 at episode 12865, frame count 8790000, time: 291375.4684920311\n",
      "best score of last 100: 910.0, running reward: 345.15 at episode 12879, frame count 8800000, time: 291733.7215321064\n",
      "best score of last 100: 910.0, running reward: 349.30 at episode 12894, frame count 8810000, time: 292093.18352007866\n",
      "best score of last 100: 910.0, running reward: 348.75 at episode 12908, frame count 8820000, time: 292451.2262570858\n",
      "best score of last 100: 910.0, running reward: 362.35 at episode 12919, frame count 8830000, time: 292809.77027606964\n",
      "best score of last 100: 910.0, running reward: 364.70 at episode 12933, frame count 8840000, time: 293171.36423397064\n",
      "best score of last 100: 890.0, running reward: 362.15 at episode 12947, frame count 8850000, time: 293529.0814960003\n",
      "best score of last 100: 890.0, running reward: 357.10 at episode 12963, frame count 8860000, time: 293893.0707581043\n",
      "best score of last 100: 805.0, running reward: 335.55 at episode 12978, frame count 8870000, time: 294252.9060370922\n",
      "best score of last 100: 805.0, running reward: 346.10 at episode 12990, frame count 8880000, time: 294611.26794695854\n",
      "best score of last 100: 805.0, running reward: 354.90 at episode 13005, frame count 8890000, time: 294977.1325559616\n",
      "best score of last 100: 805.0, running reward: 336.65 at episode 13020, frame count 8900000, time: 295334.8793120384\n",
      "best score of last 100: 805.0, running reward: 325.40 at episode 13036, frame count 8910000, time: 295691.54645109177\n",
      "best score of last 100: 630.0, running reward: 319.95 at episode 13050, frame count 8920000, time: 296049.9636991024\n",
      "best score of last 100: 630.0, running reward: 333.95 at episode 13062, frame count 8930000, time: 296408.23486304283\n",
      "best score of last 100: 630.0, running reward: 337.90 at episode 13077, frame count 8940000, time: 296769.55721211433\n",
      "best score of last 100: 630.0, running reward: 330.60 at episode 13091, frame count 8950000, time: 297129.62788391113\n",
      "best score of last 100: 630.0, running reward: 332.30 at episode 13105, frame count 8960000, time: 297492.6695609093\n",
      "best score of last 100: 955.0, running reward: 341.20 at episode 13119, frame count 8970000, time: 297851.2825360298\n",
      "best score of last 100: 955.0, running reward: 347.65 at episode 13133, frame count 8980000, time: 298212.98702692986\n",
      "best score of last 100: 955.0, running reward: 350.85 at episode 13148, frame count 8990000, time: 298573.5389459133\n",
      "best score of last 100: 955.0, running reward: 338.00 at episode 13163, frame count 9000000, time: 298932.7168331146\n",
      "best score of last 100: 955.0, running reward: 348.10 at episode 13176, frame count 9010000, time: 299291.43938708305\n",
      "best score of last 100: 955.0, running reward: 350.05 at episode 13191, frame count 9020000, time: 299650.303178072\n",
      "best score of last 100: 955.0, running reward: 344.15 at episode 13207, frame count 9030000, time: 300011.0188410282\n",
      "best score of last 100: 895.0, running reward: 343.25 at episode 13221, frame count 9040000, time: 300371.1089730263\n",
      "best score of last 100: 895.0, running reward: 346.05 at episode 13234, frame count 9050000, time: 300732.1423199177\n",
      "best score of last 100: 895.0, running reward: 351.45 at episode 13247, frame count 9060000, time: 301097.25217199326\n",
      "best score of last 100: 980.0, running reward: 360.50 at episode 13262, frame count 9070000, time: 301463.1702001095\n",
      "best score of last 100: 980.0, running reward: 348.90 at episode 13278, frame count 9080000, time: 301835.999119997\n",
      "best score of last 100: 980.0, running reward: 338.35 at episode 13294, frame count 9090000, time: 302208.5163719654\n",
      "best score of last 100: 980.0, running reward: 345.85 at episode 13306, frame count 9100000, time: 302596.35501408577\n",
      "best score of last 100: 980.0, running reward: 355.85 at episode 13320, frame count 9110000, time: 302968.47523999214\n",
      "best score of last 100: 980.0, running reward: 356.30 at episode 13334, frame count 9120000, time: 303347.8416340351\n",
      "best score of last 100: 980.0, running reward: 352.55 at episode 13348, frame count 9130000, time: 303724.21308898926\n",
      "best score of last 100: 705.0, running reward: 343.15 at episode 13364, frame count 9140000, time: 304104.5986869335\n",
      "best score of last 100: 705.0, running reward: 353.20 at episode 13378, frame count 9150000, time: 304466.8171210289\n",
      "best score of last 100: 775.0, running reward: 360.00 at episode 13392, frame count 9160000, time: 304832.971681118\n",
      "best score of last 100: 775.0, running reward: 359.45 at episode 13406, frame count 9170000, time: 305199.0475549698\n",
      "best score of last 100: 775.0, running reward: 346.80 at episode 13421, frame count 9180000, time: 305562.4632360935\n",
      "best score of last 100: 775.0, running reward: 351.75 at episode 13434, frame count 9190000, time: 305931.35926008224\n",
      "best score of last 100: 775.0, running reward: 357.75 at episode 13447, frame count 9200000, time: 306302.48755311966\n",
      "best score of last 100: 775.0, running reward: 356.90 at episode 13462, frame count 9210000, time: 306666.55327391624\n",
      "best score of last 100: 875.0, running reward: 356.95 at episode 13477, frame count 9220000, time: 307032.63475608826\n",
      "best score of last 100: 875.0, running reward: 350.75 at episode 13492, frame count 9230000, time: 307407.73034095764\n",
      "best score of last 100: 875.0, running reward: 349.85 at episode 13506, frame count 9240000, time: 307767.0567519665\n",
      "best score of last 100: 875.0, running reward: 357.75 at episode 13520, frame count 9250000, time: 308138.6128690243\n",
      "best score of last 100: 875.0, running reward: 342.75 at episode 13536, frame count 9260000, time: 308504.9277000427\n",
      "best score of last 100: 875.0, running reward: 331.15 at episode 13551, frame count 9270000, time: 308868.7950451374\n",
      "best score of last 100: 875.0, running reward: 336.40 at episode 13566, frame count 9280000, time: 309230.5821261406\n",
      "best score of last 100: 760.0, running reward: 339.50 at episode 13581, frame count 9290000, time: 309594.0286769867\n",
      "best score of last 100: 760.0, running reward: 342.45 at episode 13597, frame count 9300000, time: 309955.82288503647\n",
      "best score of last 100: 760.0, running reward: 338.05 at episode 13610, frame count 9310000, time: 310318.5522251129\n",
      "best score of last 100: 760.0, running reward: 335.60 at episode 13624, frame count 9320000, time: 310682.306030035\n",
      "best score of last 100: 940.0, running reward: 345.90 at episode 13639, frame count 9330000, time: 311045.47801208496\n",
      "best score of last 100: 940.0, running reward: 332.75 at episode 13656, frame count 9340000, time: 311407.86810207367\n",
      "best score of last 100: 940.0, running reward: 331.80 at episode 13670, frame count 9350000, time: 311771.59412908554\n",
      "best score of last 100: 940.0, running reward: 324.95 at episode 13685, frame count 9360000, time: 312134.73213505745\n",
      "best score of last 100: 940.0, running reward: 313.05 at episode 13702, frame count 9370000, time: 312498.50036907196\n",
      "best score of last 100: 940.0, running reward: 318.15 at episode 13715, frame count 9380000, time: 312866.4363090992\n",
      "best score of last 100: 940.0, running reward: 317.45 at episode 13729, frame count 9390000, time: 313229.6863229275\n",
      "best score of last 100: 745.0, running reward: 321.35 at episode 13742, frame count 9400000, time: 313595.1385500431\n",
      "best score of last 100: 1005.0, running reward: 345.40 at episode 13753, frame count 9410000, time: 313958.1768939495\n",
      "best score of last 100: 1005.0, running reward: 349.40 at episode 13769, frame count 9420000, time: 314321.16647696495\n",
      "best score of last 100: 1005.0, running reward: 367.15 at episode 13783, frame count 9430000, time: 314686.26487898827\n",
      "best score of last 100: 1005.0, running reward: 380.50 at episode 13795, frame count 9440000, time: 315048.54947400093\n",
      "best score of last 100: 1005.0, running reward: 394.95 at episode 13807, frame count 9450000, time: 315422.2675309181\n",
      "best score of last 100: 1005.0, running reward: 392.20 at episode 13822, frame count 9460000, time: 315791.5235359669\n",
      "best score of last 100: 1005.0, running reward: 391.95 at episode 13835, frame count 9470000, time: 316157.419328928\n",
      "best score of last 100: 800.0, running reward: 377.90 at episode 13848, frame count 9480000, time: 316522.8621020317\n",
      "best score of last 100: 800.0, running reward: 375.40 at episode 13864, frame count 9490000, time: 316885.06383395195\n",
      "best score of last 100: 800.0, running reward: 374.40 at episode 13878, frame count 9500000, time: 317248.5066270828\n",
      "best score of last 100: 800.0, running reward: 364.60 at episode 13892, frame count 9510000, time: 317611.0928940773\n",
      "best score of last 100: 800.0, running reward: 358.70 at episode 13906, frame count 9520000, time: 317973.25332307816\n",
      "best score of last 100: 810.0, running reward: 368.45 at episode 13917, frame count 9530000, time: 318333.01136803627\n",
      "best score of last 100: 810.0, running reward: 375.65 at episode 13932, frame count 9540000, time: 318692.7195029259\n",
      "best score of last 100: 810.0, running reward: 378.65 at episode 13946, frame count 9550000, time: 319058.35070014\n",
      "best score of last 100: 810.0, running reward: 377.90 at episode 13960, frame count 9560000, time: 319419.62050414085\n",
      "best score of last 100: 815.0, running reward: 376.50 at episode 13975, frame count 9570000, time: 319784.4363679886\n",
      "best score of last 100: 815.0, running reward: 373.05 at episode 13990, frame count 9580000, time: 320146.0501089096\n",
      "best score of last 100: 815.0, running reward: 360.50 at episode 14006, frame count 9590000, time: 320511.44225502014\n",
      "best score of last 100: 815.0, running reward: 346.00 at episode 14020, frame count 9600000, time: 320874.45714712143\n",
      "best score of last 100: 815.0, running reward: 332.85 at episode 14034, frame count 9610000, time: 321238.01004195213\n",
      "best score of last 100: 815.0, running reward: 339.00 at episode 14047, frame count 9620000, time: 321604.7279610634\n",
      "best score of last 100: 845.0, running reward: 348.20 at episode 14059, frame count 9630000, time: 321968.9471049309\n",
      "best score of last 100: 845.0, running reward: 348.70 at episode 14074, frame count 9640000, time: 322332.67554306984\n",
      "best score of last 100: 845.0, running reward: 349.75 at episode 14090, frame count 9650000, time: 322697.05591893196\n",
      "best score of last 100: 845.0, running reward: 351.25 at episode 14106, frame count 9660000, time: 323061.4385011196\n",
      "best score of last 100: 845.0, running reward: 359.85 at episode 14118, frame count 9670000, time: 323426.6245250702\n",
      "best score of last 100: 845.0, running reward: 364.45 at episode 14132, frame count 9680000, time: 323796.3211390972\n",
      "best score of last 100: 845.0, running reward: 354.25 at episode 14147, frame count 9690000, time: 324161.5749630928\n",
      "best score of last 100: 705.0, running reward: 342.35 at episode 14162, frame count 9700000, time: 324530.6385240555\n",
      "best score of last 100: 705.0, running reward: 342.60 at episode 14177, frame count 9710000, time: 324896.1002650261\n",
      "best score of last 100: 745.0, running reward: 359.85 at episode 14187, frame count 9720000, time: 325268.41056895256\n",
      "best score of last 100: 745.0, running reward: 364.10 at episode 14201, frame count 9730000, time: 325647.5463731289\n",
      "best score of last 100: 750.0, running reward: 366.45 at episode 14214, frame count 9740000, time: 326036.73265600204\n",
      "best score of last 100: 750.0, running reward: 357.75 at episode 14229, frame count 9750000, time: 326424.89904403687\n",
      "best score of last 100: 750.0, running reward: 351.70 at episode 14246, frame count 9760000, time: 326810.4234249592\n",
      "best score of last 100: 750.0, running reward: 357.40 at episode 14260, frame count 9770000, time: 327187.1865129471\n",
      "best score of last 100: 750.0, running reward: 358.35 at episode 14274, frame count 9780000, time: 327553.9284451008\n",
      "best score of last 100: 750.0, running reward: 352.40 at episode 14286, frame count 9790000, time: 327920.5926220417\n",
      "best score of last 100: 750.0, running reward: 360.20 at episode 14299, frame count 9800000, time: 328286.9844529629\n",
      "best score of last 100: 780.0, running reward: 351.60 at episode 14313, frame count 9810000, time: 328656.9183521271\n",
      "best score of last 100: 780.0, running reward: 359.85 at episode 14326, frame count 9820000, time: 329022.6417090893\n",
      "best score of last 100: 780.0, running reward: 354.20 at episode 14342, frame count 9830000, time: 329388.58443403244\n",
      "best score of last 100: 780.0, running reward: 344.50 at episode 14358, frame count 9840000, time: 329757.9145050049\n",
      "best score of last 100: 780.0, running reward: 343.25 at episode 14372, frame count 9850000, time: 330124.3325109482\n",
      "best score of last 100: 780.0, running reward: 357.45 at episode 14384, frame count 9860000, time: 330491.5210390091\n",
      "best score of last 100: 780.0, running reward: 343.10 at episode 14399, frame count 9870000, time: 330859.0649840832\n",
      "best score of last 100: 775.0, running reward: 346.25 at episode 14413, frame count 9880000, time: 331226.8269739151\n",
      "best score of last 100: 775.0, running reward: 340.75 at episode 14428, frame count 9890000, time: 331597.80777692795\n",
      "best score of last 100: 775.0, running reward: 349.75 at episode 14442, frame count 9900000, time: 331964.8907020092\n",
      "best score of last 100: 775.0, running reward: 364.65 at episode 14456, frame count 9910000, time: 332336.349118948\n",
      "best score of last 100: 775.0, running reward: 369.90 at episode 14470, frame count 9920000, time: 332705.68744397163\n",
      "best score of last 100: 775.0, running reward: 346.45 at episode 14485, frame count 9930000, time: 333077.2898430824\n",
      "best score of last 100: 775.0, running reward: 349.85 at episode 14499, frame count 9940000, time: 333447.5023190975\n",
      "best score of last 100: 775.0, running reward: 343.45 at episode 14512, frame count 9950000, time: 333814.26547312737\n",
      "best score of last 100: 705.0, running reward: 351.55 at episode 14526, frame count 9960000, time: 334181.17690205574\n",
      "best score of last 100: 705.0, running reward: 346.85 at episode 14540, frame count 9970000, time: 334547.354213953\n",
      "best score of last 100: 685.0, running reward: 337.80 at episode 14554, frame count 9980000, time: 334914.9066851139\n",
      "best score of last 100: 665.0, running reward: 333.35 at episode 14568, frame count 9990000, time: 335283.3970401287\n",
      "best score of last 100: 665.0, running reward: 333.75 at episode 14582, frame count 10000000, time: 335653.55365014076\n",
      "Stopped at frame 10000010!\n",
      "Stopped at frame 10000502!\n",
      "Stopped at frame 10000988!\n",
      "Stopped at frame 10001592!\n",
      "Stopped at frame 10002346!\n",
      "Stopped at frame 10003158!\n",
      "Stopped at frame 10003998!\n",
      "Stopped at frame 10004334!\n",
      "Stopped at frame 10004808!\n",
      "Stopped at frame 10005236!\n",
      "Stopped at frame 10005652!\n",
      "Stopped at frame 10006117!\n",
      "Stopped at frame 10007077!\n",
      "Stopped at frame 10007922!\n",
      "Stopped at frame 10008397!\n",
      "Stopped at frame 10008873!\n",
      "Stopped at frame 10009262!\n",
      "Stopped at frame 10009722!\n",
      "best score of last 100: 670.0, running reward: 321.95 at episode 14600, frame count 10010000, time: 336020.1009209156\n",
      "Stopped at frame 10010367!\n",
      "Stopped at frame 10010818!\n",
      "Stopped at frame 10011196!\n",
      "Stopped at frame 10012380!\n",
      "Stopped at frame 10012741!\n",
      "Stopped at frame 10013455!\n",
      "Stopped at frame 10014365!\n",
      "Stopped at frame 10014639!\n",
      "Stopped at frame 10015355!\n",
      "Stopped at frame 10015718!\n",
      "Stopped at frame 10016368!\n",
      "Stopped at frame 10017432!\n",
      "Stopped at frame 10017885!\n",
      "Stopped at frame 10018366!\n",
      "Stopped at frame 10018777!\n",
      "Stopped at frame 10019439!\n",
      "Stopped at frame 10019929!\n",
      "best score of last 100: 985.0, running reward: 314.30 at episode 14617, frame count 10020000, time: 336389.8479299545\n",
      "Stopped at frame 10020400!\n",
      "Stopped at frame 10021289!\n",
      "Stopped at frame 10021986!\n",
      "Stopped at frame 10022620!\n",
      "Stopped at frame 10023256!\n",
      "Stopped at frame 10024433!\n",
      "Stopped at frame 10024971!\n",
      "Stopped at frame 10025833!\n",
      "Stopped at frame 10026322!\n",
      "Stopped at frame 10026926!\n",
      "Stopped at frame 10027823!\n",
      "Stopped at frame 10028607!\n",
      "Stopped at frame 10029218!\n",
      "best score of last 100: 985.0, running reward: 316.35 at episode 14630, frame count 10030000, time: 336755.1520910263\n",
      "Stopped at frame 10030725!\n",
      "Stopped at frame 10031321!\n",
      "Stopped at frame 10031691!\n",
      "Stopped at frame 10032730!\n",
      "Stopped at frame 10033493!\n",
      "Stopped at frame 10033876!\n",
      "Stopped at frame 10034346!\n",
      "Stopped at frame 10034895!\n",
      "Stopped at frame 10035645!\n",
      "Stopped at frame 10036590!\n",
      "Stopped at frame 10037225!\n",
      "Stopped at frame 10038018!\n",
      "Stopped at frame 10038495!\n",
      "Stopped at frame 10039350!\n",
      "best score of last 100: 985.0, running reward: 325.65 at episode 14644, frame count 10040000, time: 337124.99659991264\n",
      "Stopped at frame 10040029!\n",
      "Stopped at frame 10040476!\n",
      "Stopped at frame 10041078!\n",
      "Stopped at frame 10041733!\n",
      "Stopped at frame 10042467!\n",
      "Stopped at frame 10043004!\n",
      "Stopped at frame 10043873!\n",
      "Stopped at frame 10044306!\n",
      "Stopped at frame 10045366!\n",
      "Stopped at frame 10046295!\n",
      "Stopped at frame 10047008!\n",
      "Stopped at frame 10048715!\n",
      "Stopped at frame 10049392!\n",
      "best score of last 100: 985.0, running reward: 326.30 at episode 14657, frame count 10050000, time: 337492.5997159481\n",
      "Stopped at frame 10050215!\n",
      "Stopped at frame 10050655!\n",
      "Stopped at frame 10051182!\n",
      "Stopped at frame 10051705!\n",
      "Stopped at frame 10052142!\n",
      "Stopped at frame 10053120!\n",
      "Stopped at frame 10054338!\n",
      "Stopped at frame 10054807!\n",
      "Stopped at frame 10055487!\n",
      "Stopped at frame 10056532!\n",
      "Stopped at frame 10057721!\n",
      "Stopped at frame 10058187!\n",
      "Stopped at frame 10058604!\n",
      "Stopped at frame 10059182!\n",
      "Stopped at frame 10059927!\n",
      "best score of last 100: 985.0, running reward: 321.85 at episode 14672, frame count 10060000, time: 337861.1645989418\n",
      "Stopped at frame 10060530!\n",
      "Stopped at frame 10061515!\n",
      "Stopped at frame 10062318!\n",
      "Stopped at frame 10062672!\n",
      "Stopped at frame 10063046!\n",
      "Stopped at frame 10064355!\n",
      "Stopped at frame 10064965!\n",
      "Stopped at frame 10065720!\n",
      "Stopped at frame 10067067!\n",
      "Stopped at frame 10067637!\n",
      "Stopped at frame 10068855!\n",
      "Stopped at frame 10069655!\n",
      "best score of last 100: 985.0, running reward: 331.45 at episode 14684, frame count 10070000, time: 338229.7408800125\n",
      "Stopped at frame 10070429!\n",
      "Stopped at frame 10070876!\n",
      "Stopped at frame 10071582!\n",
      "Stopped at frame 10072055!\n",
      "Stopped at frame 10072544!\n",
      "Stopped at frame 10072986!\n",
      "Stopped at frame 10073854!\n",
      "Stopped at frame 10074711!\n",
      "Stopped at frame 10075324!\n",
      "Stopped at frame 10076135!\n",
      "Stopped at frame 10077066!\n",
      "Stopped at frame 10077590!\n",
      "Stopped at frame 10077885!\n",
      "Stopped at frame 10078568!\n",
      "Stopped at frame 10079398!\n",
      "best score of last 100: 985.0, running reward: 333.40 at episode 14699, frame count 10080000, time: 338600.44272613525\n",
      "Stopped at frame 10080403!\n",
      "Stopped at frame 10080866!\n",
      "Stopped at frame 10081472!\n",
      "Stopped at frame 10082505!\n",
      "Stopped at frame 10082928!\n",
      "Stopped at frame 10083411!\n",
      "Stopped at frame 10083835!\n",
      "Stopped at frame 10084573!\n",
      "Stopped at frame 10085285!\n",
      "Stopped at frame 10085879!\n",
      "Stopped at frame 10086581!\n",
      "Stopped at frame 10087288!\n",
      "Stopped at frame 10088520!\n",
      "Stopped at frame 10088945!\n",
      "Stopped at frame 10089594!\n",
      "best score of last 100: 660.0, running reward: 332.25 at episode 14714, frame count 10090000, time: 338969.2896780968\n",
      "Stopped at frame 10090247!\n",
      "Stopped at frame 10090913!\n",
      "Stopped at frame 10091797!\n",
      "Stopped at frame 10092276!\n",
      "Stopped at frame 10092752!\n",
      "Stopped at frame 10093384!\n",
      "Stopped at frame 10094073!\n",
      "Stopped at frame 10094751!\n",
      "Stopped at frame 10095376!\n",
      "Stopped at frame 10096224!\n",
      "Stopped at frame 10096813!\n",
      "Stopped at frame 10097827!\n",
      "Stopped at frame 10098690!\n",
      "Stopped at frame 10099391!\n",
      "best score of last 100: 775.0, running reward: 334.65 at episode 14728, frame count 10100000, time: 339335.3832440376\n",
      "Stopped at frame 10100001!\n",
      "Stopped at frame 10100643!\n",
      "Stopped at frame 10101650!\n",
      "Stopped at frame 10102054!\n",
      "Stopped at frame 10102613!\n",
      "Stopped at frame 10103322!\n",
      "Stopped at frame 10103942!\n",
      "Stopped at frame 10104546!\n",
      "Stopped at frame 10105341!\n",
      "Stopped at frame 10106131!\n",
      "Stopped at frame 10106842!\n",
      "Stopped at frame 10107452!\n",
      "Stopped at frame 10108147!\n",
      "Stopped at frame 10108868!\n",
      "Stopped at frame 10109546!\n",
      "best score of last 100: 775.0, running reward: 336.85 at episode 14743, frame count 10110000, time: 339703.28553414345\n",
      "Stopped at frame 10110658!\n",
      "Stopped at frame 10111567!\n",
      "Stopped at frame 10112348!\n",
      "Stopped at frame 10113193!\n",
      "Stopped at frame 10113612!\n",
      "Stopped at frame 10114433!\n",
      "Stopped at frame 10115119!\n",
      "Stopped at frame 10115896!\n",
      "Stopped at frame 10116548!\n",
      "Stopped at frame 10117124!\n",
      "Stopped at frame 10118395!\n",
      "Stopped at frame 10119209!\n",
      "Stopped at frame 10119616!\n",
      "best score of last 100: 775.0, running reward: 336.00 at episode 14756, frame count 10120000, time: 340068.7308869362\n",
      "Stopped at frame 10120370!\n",
      "Stopped at frame 10120811!\n",
      "Stopped at frame 10121173!\n",
      "Stopped at frame 10121872!\n",
      "Stopped at frame 10122428!\n",
      "Stopped at frame 10123216!\n",
      "Stopped at frame 10123998!\n",
      "Stopped at frame 10124930!\n",
      "Stopped at frame 10125646!\n",
      "Stopped at frame 10126061!\n",
      "Stopped at frame 10126882!\n",
      "Stopped at frame 10127438!\n",
      "Stopped at frame 10127958!\n",
      "Stopped at frame 10128550!\n",
      "Stopped at frame 10129108!\n",
      "Stopped at frame 10129752!\n",
      "best score of last 100: 775.0, running reward: 340.20 at episode 14772, frame count 10130000, time: 340435.1376950741\n",
      "Stopped at frame 10130435!\n",
      "Stopped at frame 10131316!\n",
      "Stopped at frame 10132367!\n",
      "Stopped at frame 10133103!\n",
      "Stopped at frame 10134061!\n",
      "Stopped at frame 10134613!\n",
      "Stopped at frame 10135175!\n",
      "Stopped at frame 10135785!\n",
      "Stopped at frame 10136674!\n",
      "Stopped at frame 10137630!\n",
      "Stopped at frame 10138426!\n",
      "Stopped at frame 10139107!\n",
      "Stopped at frame 10139556!\n",
      "best score of last 100: 775.0, running reward: 339.60 at episode 14785, frame count 10140000, time: 340805.92053198814\n",
      "Stopped at frame 10140374!\n",
      "Stopped at frame 10140977!\n",
      "Stopped at frame 10142088!\n",
      "Stopped at frame 10142556!\n",
      "Stopped at frame 10143092!\n",
      "Stopped at frame 10143673!\n",
      "Stopped at frame 10144456!\n",
      "Stopped at frame 10145740!\n",
      "Stopped at frame 10146114!\n",
      "Stopped at frame 10146694!\n",
      "Stopped at frame 10147429!\n",
      "Stopped at frame 10147926!\n",
      "Stopped at frame 10148955!\n",
      "Stopped at frame 10149423!\n",
      "Stopped at frame 10149847!\n",
      "best score of last 100: 775.0, running reward: 350.05 at episode 14800, frame count 10150000, time: 341170.9419529438\n",
      "Stopped at frame 10150656!\n",
      "Stopped at frame 10151237!\n",
      "Stopped at frame 10152261!\n",
      "Stopped at frame 10152740!\n",
      "Stopped at frame 10153122!\n",
      "Stopped at frame 10154171!\n",
      "Stopped at frame 10154970!\n",
      "Stopped at frame 10155679!\n",
      "Stopped at frame 10156687!\n",
      "Stopped at frame 10157480!\n",
      "Stopped at frame 10158433!\n",
      "Stopped at frame 10158947!\n",
      "Stopped at frame 10159662!\n",
      "best score of last 100: 775.0, running reward: 366.15 at episode 14813, frame count 10160000, time: 341544.93907094\n",
      "Stopped at frame 10160281!\n",
      "Stopped at frame 10161388!\n",
      "Stopped at frame 10161884!\n",
      "Stopped at frame 10162502!\n",
      "Stopped at frame 10163223!\n",
      "Stopped at frame 10163821!\n",
      "Stopped at frame 10164440!\n",
      "Stopped at frame 10164790!\n",
      "Stopped at frame 10165492!\n",
      "Stopped at frame 10166183!\n",
      "Stopped at frame 10167480!\n",
      "Stopped at frame 10168461!\n",
      "Stopped at frame 10169296!\n",
      "best score of last 100: 755.0, running reward: 369.05 at episode 14826, frame count 10170000, time: 341915.2485320568\n",
      "Stopped at frame 10170109!\n",
      "Stopped at frame 10170726!\n",
      "Stopped at frame 10171550!\n",
      "Stopped at frame 10172408!\n",
      "Stopped at frame 10172784!\n",
      "Stopped at frame 10173590!\n",
      "Stopped at frame 10174483!\n",
      "Stopped at frame 10174835!\n",
      "Stopped at frame 10175695!\n",
      "Stopped at frame 10176319!\n",
      "Stopped at frame 10177510!\n",
      "Stopped at frame 10178245!\n",
      "Stopped at frame 10178867!\n",
      "Stopped at frame 10179772!\n",
      "best score of last 100: 755.0, running reward: 379.75 at episode 14840, frame count 10180000, time: 342285.6797199249\n",
      "Stopped at frame 10180815!\n",
      "Stopped at frame 10181562!\n",
      "Stopped at frame 10182190!\n",
      "Stopped at frame 10182616!\n",
      "Stopped at frame 10183285!\n",
      "Stopped at frame 10183970!\n",
      "Stopped at frame 10184668!\n",
      "Stopped at frame 10185145!\n",
      "Stopped at frame 10185618!\n",
      "Stopped at frame 10186359!\n",
      "Stopped at frame 10187199!\n",
      "Stopped at frame 10188294!\n",
      "Stopped at frame 10188725!\n",
      "Stopped at frame 10189564!\n",
      "best score of last 100: 755.0, running reward: 372.35 at episode 14854, frame count 10190000, time: 342656.4163520336\n",
      "Stopped at frame 10191206!\n",
      "Stopped at frame 10192398!\n",
      "Stopped at frame 10192888!\n",
      "Stopped at frame 10193611!\n",
      "Stopped at frame 10194225!\n",
      "Stopped at frame 10194722!\n",
      "Stopped at frame 10195481!\n",
      "Stopped at frame 10196192!\n",
      "Stopped at frame 10196813!\n",
      "Stopped at frame 10197584!\n",
      "Stopped at frame 10198315!\n",
      "Stopped at frame 10199031!\n",
      "Stopped at frame 10199776!\n",
      "best score of last 100: 755.0, running reward: 381.85 at episode 14867, frame count 10200000, time: 343029.4633779526\n",
      "Stopped at frame 10200194!\n",
      "Stopped at frame 10201001!\n",
      "Stopped at frame 10201644!\n",
      "Stopped at frame 10202222!\n",
      "Stopped at frame 10202824!\n",
      "Stopped at frame 10203611!\n",
      "Stopped at frame 10204201!\n",
      "Stopped at frame 10204942!\n",
      "Stopped at frame 10205551!\n",
      "Stopped at frame 10206119!\n",
      "Stopped at frame 10206741!\n",
      "Stopped at frame 10207468!\n",
      "Stopped at frame 10208663!\n",
      "Stopped at frame 10209627!\n",
      "best score of last 100: 845.0, running reward: 378.35 at episode 14881, frame count 10210000, time: 343400.4515221119\n",
      "Stopped at frame 10210250!\n",
      "Stopped at frame 10210965!\n",
      "Stopped at frame 10211460!\n",
      "Stopped at frame 10211846!\n",
      "Stopped at frame 10212275!\n",
      "Stopped at frame 10213147!\n",
      "Stopped at frame 10213818!\n",
      "Stopped at frame 10214550!\n",
      "Stopped at frame 10215083!\n",
      "Stopped at frame 10215500!\n",
      "Stopped at frame 10216088!\n",
      "Stopped at frame 10216594!\n",
      "Stopped at frame 10217662!\n",
      "Stopped at frame 10218906!\n",
      "Stopped at frame 10219675!\n",
      "best score of last 100: 845.0, running reward: 369.80 at episode 14896, frame count 10220000, time: 343771.2734630108\n",
      "Stopped at frame 10220368!\n",
      "Stopped at frame 10221456!\n",
      "Stopped at frame 10221999!\n",
      "Stopped at frame 10223209!\n",
      "Stopped at frame 10224231!\n",
      "Stopped at frame 10224867!\n",
      "Stopped at frame 10225452!\n",
      "Stopped at frame 10226614!\n",
      "Stopped at frame 10227352!\n",
      "Stopped at frame 10227737!\n",
      "Stopped at frame 10228253!\n",
      "Stopped at frame 10228735!\n",
      "Stopped at frame 10229112!\n",
      "Stopped at frame 10229798!\n",
      "best score of last 100: 845.0, running reward: 361.25 at episode 14910, frame count 10230000, time: 344151.0198459625\n",
      "Stopped at frame 10230489!\n",
      "Stopped at frame 10230949!\n",
      "Stopped at frame 10231756!\n",
      "Stopped at frame 10232546!\n",
      "Stopped at frame 10232956!\n",
      "Stopped at frame 10233681!\n",
      "Stopped at frame 10234378!\n",
      "Stopped at frame 10235102!\n",
      "Stopped at frame 10235583!\n",
      "Stopped at frame 10236436!\n",
      "Stopped at frame 10236880!\n",
      "Stopped at frame 10238004!\n",
      "Stopped at frame 10238414!\n",
      "Stopped at frame 10239056!\n",
      "Stopped at frame 10239496!\n",
      "Stopped at frame 10239949!\n",
      "best score of last 100: 845.0, running reward: 346.65 at episode 14926, frame count 10240000, time: 344525.5007901192\n",
      "Stopped at frame 10240761!\n",
      "Stopped at frame 10241348!\n",
      "Stopped at frame 10242338!\n",
      "Stopped at frame 10242937!\n",
      "Stopped at frame 10243421!\n",
      "Stopped at frame 10243869!\n",
      "Stopped at frame 10244766!\n",
      "Stopped at frame 10245639!\n",
      "Stopped at frame 10246245!\n",
      "Stopped at frame 10247297!\n",
      "Stopped at frame 10248055!\n",
      "Stopped at frame 10249101!\n",
      "best score of last 100: 845.0, running reward: 344.35 at episode 14938, frame count 10250000, time: 344899.3697690964\n",
      "Stopped at frame 10250418!\n",
      "Stopped at frame 10251186!\n",
      "Stopped at frame 10251735!\n",
      "Stopped at frame 10252371!\n",
      "Stopped at frame 10253051!\n",
      "Stopped at frame 10254058!\n",
      "Stopped at frame 10254765!\n",
      "Stopped at frame 10256003!\n",
      "Stopped at frame 10256760!\n",
      "Stopped at frame 10257996!\n",
      "Stopped at frame 10258449!\n",
      "Stopped at frame 10259709!\n",
      "best score of last 100: 845.0, running reward: 350.05 at episode 14950, frame count 10260000, time: 345273.15846300125\n",
      "Stopped at frame 10260642!\n",
      "Stopped at frame 10261124!\n",
      "Stopped at frame 10261711!\n",
      "Stopped at frame 10262416!\n",
      "Stopped at frame 10263146!\n",
      "Stopped at frame 10264142!\n",
      "Stopped at frame 10264586!\n",
      "Stopped at frame 10265165!\n",
      "Stopped at frame 10265580!\n",
      "Stopped at frame 10266230!\n",
      "Stopped at frame 10266720!\n",
      "Stopped at frame 10267449!\n",
      "Stopped at frame 10267979!\n",
      "Stopped at frame 10268689!\n",
      "Stopped at frame 10269513!\n",
      "best score of last 100: 845.0, running reward: 339.90 at episode 14965, frame count 10270000, time: 345647.67097592354\n",
      "Stopped at frame 10270259!\n",
      "Stopped at frame 10270884!\n",
      "Stopped at frame 10271393!\n",
      "Stopped at frame 10272425!\n",
      "Stopped at frame 10273059!\n",
      "Stopped at frame 10274059!\n",
      "Stopped at frame 10274642!\n",
      "Stopped at frame 10275281!\n",
      "Stopped at frame 10275717!\n",
      "Stopped at frame 10276392!\n",
      "Stopped at frame 10277452!\n",
      "Stopped at frame 10277932!\n",
      "Stopped at frame 10278790!\n",
      "Stopped at frame 10279164!\n",
      "Stopped at frame 10279613!\n",
      "best score of last 100: 720.0, running reward: 339.90 at episode 14980, frame count 10280000, time: 346021.59266901016\n",
      "Stopped at frame 10280115!\n",
      "Stopped at frame 10280859!\n",
      "Stopped at frame 10281577!\n",
      "Stopped at frame 10282260!\n",
      "Stopped at frame 10283175!\n",
      "Stopped at frame 10283814!\n",
      "Stopped at frame 10284269!\n",
      "Stopped at frame 10284846!\n",
      "Stopped at frame 10285261!\n",
      "Stopped at frame 10285866!\n",
      "Stopped at frame 10286301!\n",
      "Stopped at frame 10287068!\n",
      "Stopped at frame 10287963!\n",
      "Stopped at frame 10288862!\n",
      "Stopped at frame 10289548!\n",
      "Stopped at frame 10289989!\n",
      "best score of last 100: 730.0, running reward: 343.00 at episode 14996, frame count 10290000, time: 346394.0961239338\n",
      "Stopped at frame 10290758!\n",
      "Stopped at frame 10291096!\n",
      "Stopped at frame 10291513!\n",
      "Stopped at frame 10292106!\n",
      "Stopped at frame 10292624!\n",
      "Stopped at frame 10293283!\n",
      "Stopped at frame 10293995!\n",
      "Stopped at frame 10294831!\n",
      "Stopped at frame 10295482!\n",
      "Stopped at frame 10296314!\n",
      "Stopped at frame 10297112!\n",
      "Stopped at frame 10297723!\n",
      "Stopped at frame 10298306!\n",
      "Stopped at frame 10299795!\n",
      "best score of last 100: 730.0, running reward: 343.25 at episode 15010, frame count 10300000, time: 346765.4333200455\n",
      "Stopped at frame 10300783!\n",
      "Stopped at frame 10301350!\n",
      "Stopped at frame 10302599!\n",
      "Stopped at frame 10303355!\n",
      "Stopped at frame 10304126!\n",
      "Stopped at frame 10304908!\n",
      "Stopped at frame 10305441!\n",
      "Stopped at frame 10306188!\n",
      "Stopped at frame 10307256!\n",
      "Stopped at frame 10308164!\n",
      "Stopped at frame 10309009!\n",
      "Stopped at frame 10309597!\n",
      "best score of last 100: 730.0, running reward: 353.55 at episode 15022, frame count 10310000, time: 347137.3017859459\n",
      "Stopped at frame 10310666!\n",
      "Stopped at frame 10311461!\n",
      "Stopped at frame 10311828!\n",
      "Stopped at frame 10312620!\n",
      "Stopped at frame 10313107!\n",
      "Stopped at frame 10313736!\n",
      "Stopped at frame 10314664!\n",
      "Stopped at frame 10315120!\n",
      "Stopped at frame 10315606!\n",
      "Stopped at frame 10316498!\n",
      "Stopped at frame 10316898!\n",
      "Stopped at frame 10317384!\n",
      "Stopped at frame 10317945!\n",
      "Stopped at frame 10318703!\n",
      "Stopped at frame 10319277!\n",
      "Stopped at frame 10319741!\n",
      "best score of last 100: 730.0, running reward: 341.80 at episode 15038, frame count 10320000, time: 347509.31913614273\n",
      "Stopped at frame 10320682!\n",
      "Stopped at frame 10321477!\n",
      "Stopped at frame 10322660!\n",
      "Stopped at frame 10323328!\n",
      "Stopped at frame 10324152!\n",
      "Stopped at frame 10324523!\n",
      "Stopped at frame 10325342!\n",
      "Stopped at frame 10326062!\n",
      "Stopped at frame 10326788!\n",
      "Stopped at frame 10327380!\n",
      "Stopped at frame 10327866!\n",
      "Stopped at frame 10328447!\n",
      "Stopped at frame 10329416!\n",
      "best score of last 100: 780.0, running reward: 346.00 at episode 15051, frame count 10330000, time: 347886.23070406914\n",
      "Stopped at frame 10330347!\n",
      "Stopped at frame 10330798!\n",
      "Stopped at frame 10331516!\n",
      "Stopped at frame 10332414!\n",
      "Stopped at frame 10333434!\n",
      "Stopped at frame 10334103!\n",
      "Stopped at frame 10334474!\n",
      "Stopped at frame 10334959!\n",
      "Stopped at frame 10335833!\n",
      "Stopped at frame 10336451!\n",
      "Stopped at frame 10337014!\n",
      "Stopped at frame 10337577!\n",
      "Stopped at frame 10337933!\n",
      "Stopped at frame 10338597!\n",
      "Stopped at frame 10339613!\n",
      "best score of last 100: 780.0, running reward: 345.00 at episode 15066, frame count 10340000, time: 348263.6863000393\n",
      "Stopped at frame 10340236!\n",
      "Stopped at frame 10340955!\n",
      "Stopped at frame 10341757!\n",
      "Stopped at frame 10342786!\n",
      "Stopped at frame 10343238!\n",
      "Stopped at frame 10343708!\n",
      "Stopped at frame 10344387!\n",
      "Stopped at frame 10345295!\n",
      "Stopped at frame 10345905!\n",
      "Stopped at frame 10347012!\n",
      "Stopped at frame 10347809!\n",
      "Stopped at frame 10348199!\n",
      "Stopped at frame 10348974!\n",
      "Stopped at frame 10349494!\n",
      "best score of last 100: 780.0, running reward: 347.35 at episode 15080, frame count 10350000, time: 348638.6182420254\n",
      "Stopped at frame 10350016!\n",
      "Stopped at frame 10350511!\n",
      "Stopped at frame 10351186!\n",
      "Stopped at frame 10351652!\n",
      "Stopped at frame 10352532!\n",
      "Stopped at frame 10353258!\n",
      "Stopped at frame 10353775!\n",
      "Stopped at frame 10354176!\n",
      "Stopped at frame 10354976!\n",
      "Stopped at frame 10355468!\n",
      "Stopped at frame 10356341!\n",
      "Stopped at frame 10356796!\n",
      "Stopped at frame 10357273!\n",
      "Stopped at frame 10358106!\n",
      "Stopped at frame 10358949!\n",
      "Stopped at frame 10359475!\n",
      "best score of last 100: 780.0, running reward: 346.40 at episode 15096, frame count 10360000, time: 349009.15576410294\n",
      "Stopped at frame 10360404!\n",
      "Stopped at frame 10361735!\n",
      "Stopped at frame 10362750!\n",
      "Stopped at frame 10363548!\n",
      "Stopped at frame 10364025!\n",
      "Stopped at frame 10364425!\n",
      "Stopped at frame 10364843!\n",
      "Stopped at frame 10365106!\n",
      "Stopped at frame 10366343!\n",
      "Stopped at frame 10367076!\n",
      "Stopped at frame 10367557!\n",
      "Stopped at frame 10368281!\n",
      "Stopped at frame 10369042!\n",
      "Stopped at frame 10369651!\n",
      "best score of last 100: 780.0, running reward: 346.45 at episode 15110, frame count 10370000, time: 349382.60841202736\n",
      "Stopped at frame 10370372!\n",
      "Stopped at frame 10371128!\n",
      "Stopped at frame 10371740!\n",
      "Stopped at frame 10372614!\n",
      "Stopped at frame 10373143!\n",
      "Stopped at frame 10373962!\n",
      "Stopped at frame 10375014!\n",
      "Stopped at frame 10376144!\n",
      "Stopped at frame 10376786!\n",
      "Stopped at frame 10377502!\n",
      "Stopped at frame 10378147!\n",
      "Stopped at frame 10378567!\n",
      "Stopped at frame 10378932!\n",
      "best score of last 100: 780.0, running reward: 338.80 at episode 15123, frame count 10380000, time: 349750.61339497566\n",
      "Stopped at frame 10380157!\n",
      "Stopped at frame 10380805!\n",
      "Stopped at frame 10381878!\n",
      "Stopped at frame 10382409!\n",
      "Stopped at frame 10382772!\n",
      "Stopped at frame 10383288!\n",
      "Stopped at frame 10384001!\n",
      "Stopped at frame 10384731!\n",
      "Stopped at frame 10385218!\n",
      "Stopped at frame 10385507!\n",
      "Stopped at frame 10385864!\n",
      "Stopped at frame 10386621!\n",
      "Stopped at frame 10387293!\n",
      "Stopped at frame 10388225!\n",
      "Stopped at frame 10389102!\n",
      "Stopped at frame 10389556!\n",
      "best score of last 100: 780.0, running reward: 339.15 at episode 15139, frame count 10390000, time: 350118.96251392365\n",
      "Stopped at frame 10390241!\n",
      "Stopped at frame 10391378!\n",
      "Stopped at frame 10391857!\n",
      "Stopped at frame 10392329!\n",
      "Stopped at frame 10393235!\n",
      "Stopped at frame 10393772!\n",
      "Stopped at frame 10394362!\n",
      "Stopped at frame 10394953!\n",
      "Stopped at frame 10395578!\n",
      "Stopped at frame 10396131!\n",
      "Stopped at frame 10396654!\n",
      "Stopped at frame 10397183!\n",
      "Stopped at frame 10397686!\n",
      "Stopped at frame 10398234!\n",
      "Stopped at frame 10399238!\n",
      "Stopped at frame 10399866!\n",
      "best score of last 100: 675.0, running reward: 323.80 at episode 15155, frame count 10400000, time: 350490.99940896034\n",
      "Stopped at frame 10400454!\n",
      "Stopped at frame 10401032!\n",
      "Stopped at frame 10402040!\n",
      "Stopped at frame 10402703!\n",
      "Stopped at frame 10403251!\n",
      "Stopped at frame 10403968!\n",
      "Stopped at frame 10404712!\n",
      "Stopped at frame 10405229!\n",
      "Stopped at frame 10405976!\n",
      "Stopped at frame 10406854!\n",
      "Stopped at frame 10407338!\n",
      "Stopped at frame 10408122!\n",
      "Stopped at frame 10408917!\n",
      "Stopped at frame 10409660!\n",
      "best score of last 100: 675.0, running reward: 331.35 at episode 15169, frame count 10410000, time: 350860.1907610893\n",
      "Stopped at frame 10410074!\n",
      "Stopped at frame 10411037!\n",
      "Stopped at frame 10412306!\n",
      "Stopped at frame 10412895!\n",
      "Stopped at frame 10413538!\n",
      "Stopped at frame 10414367!\n",
      "Stopped at frame 10414734!\n",
      "Stopped at frame 10415977!\n",
      "Stopped at frame 10416617!\n",
      "Stopped at frame 10417340!\n",
      "Stopped at frame 10417965!\n",
      "Stopped at frame 10418520!\n",
      "Stopped at frame 10419154!\n",
      "Stopped at frame 10419551!\n",
      "best score of last 100: 640.0, running reward: 327.80 at episode 15183, frame count 10420000, time: 351232.2524609566\n",
      "Stopped at frame 10420081!\n",
      "Stopped at frame 10420898!\n",
      "Stopped at frame 10421872!\n",
      "Stopped at frame 10422476!\n",
      "Stopped at frame 10423578!\n",
      "Stopped at frame 10424616!\n",
      "Stopped at frame 10425247!\n",
      "Stopped at frame 10425952!\n",
      "Stopped at frame 10427015!\n",
      "Stopped at frame 10427382!\n",
      "Stopped at frame 10428247!\n",
      "Stopped at frame 10429232!\n",
      "Stopped at frame 10429823!\n",
      "best score of last 100: 865.0, running reward: 341.90 at episode 15196, frame count 10430000, time: 351602.86073613167\n",
      "Stopped at frame 10430430!\n",
      "Stopped at frame 10431262!\n",
      "Stopped at frame 10432141!\n",
      "Stopped at frame 10432699!\n",
      "Stopped at frame 10432982!\n",
      "Stopped at frame 10433528!\n",
      "Stopped at frame 10434193!\n",
      "Stopped at frame 10435114!\n",
      "Stopped at frame 10435880!\n",
      "Stopped at frame 10436384!\n",
      "Stopped at frame 10436995!\n",
      "Stopped at frame 10437701!\n",
      "Stopped at frame 10438370!\n",
      "Stopped at frame 10438811!\n",
      "Stopped at frame 10439806!\n",
      "best score of last 100: 865.0, running reward: 342.50 at episode 15211, frame count 10440000, time: 351975.9402720928\n",
      "Stopped at frame 10440652!\n",
      "Stopped at frame 10442059!\n",
      "Stopped at frame 10442634!\n",
      "Stopped at frame 10443757!\n",
      "Stopped at frame 10444885!\n",
      "Stopped at frame 10445545!\n",
      "Stopped at frame 10446249!\n",
      "Stopped at frame 10446566!\n",
      "Stopped at frame 10447263!\n",
      "Stopped at frame 10447886!\n",
      "Stopped at frame 10448561!\n",
      "Stopped at frame 10449275!\n",
      "Stopped at frame 10449885!\n",
      "best score of last 100: 865.0, running reward: 349.25 at episode 15224, frame count 10450000, time: 352352.6266131401\n",
      "Stopped at frame 10450932!\n",
      "Stopped at frame 10451667!\n",
      "Stopped at frame 10452054!\n",
      "Stopped at frame 10452450!\n",
      "Stopped at frame 10453667!\n",
      "Stopped at frame 10453953!\n",
      "Stopped at frame 10454419!\n",
      "Stopped at frame 10455488!\n",
      "Stopped at frame 10456279!\n",
      "Stopped at frame 10456971!\n",
      "Stopped at frame 10457584!\n",
      "Stopped at frame 10458362!\n",
      "Stopped at frame 10458785!\n",
      "Stopped at frame 10459507!\n",
      "best score of last 100: 865.0, running reward: 356.35 at episode 15238, frame count 10460000, time: 352728.61540198326\n",
      "Stopped at frame 10460444!\n",
      "Stopped at frame 10461190!\n",
      "Stopped at frame 10461627!\n",
      "Stopped at frame 10462252!\n",
      "Stopped at frame 10463464!\n",
      "Stopped at frame 10464056!\n",
      "Stopped at frame 10465014!\n",
      "Stopped at frame 10465548!\n",
      "Stopped at frame 10466472!\n",
      "Stopped at frame 10466960!\n",
      "Stopped at frame 10468016!\n",
      "Stopped at frame 10468653!\n",
      "Stopped at frame 10469255!\n",
      "best score of last 100: 865.0, running reward: 366.90 at episode 15251, frame count 10470000, time: 353104.5404429436\n",
      "Stopped at frame 10470328!\n",
      "Stopped at frame 10471227!\n",
      "Stopped at frame 10471963!\n",
      "Stopped at frame 10472425!\n",
      "Stopped at frame 10472835!\n",
      "Stopped at frame 10473683!\n",
      "Stopped at frame 10474323!\n",
      "Stopped at frame 10474999!\n",
      "Stopped at frame 10475381!\n",
      "Stopped at frame 10475996!\n",
      "Stopped at frame 10476640!\n",
      "Stopped at frame 10477018!\n",
      "Stopped at frame 10477539!\n",
      "Stopped at frame 10477857!\n",
      "Stopped at frame 10478447!\n",
      "Stopped at frame 10478975!\n",
      "Stopped at frame 10479626!\n",
      "Stopped at frame 10479981!\n",
      "best score of last 100: 865.0, running reward: 359.45 at episode 15269, frame count 10480000, time: 353481.8954730034\n",
      "Stopped at frame 10480926!\n",
      "Stopped at frame 10481683!\n",
      "Stopped at frame 10482778!\n",
      "Stopped at frame 10483401!\n",
      "Stopped at frame 10483998!\n",
      "Stopped at frame 10484458!\n",
      "Stopped at frame 10485279!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 67\u001b[0m\n\u001b[0;32m     61\u001b[0m done_sample \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m     62\u001b[0m     [\u001b[38;5;28mfloat\u001b[39m(done_history[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m indices], dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[0;32m     63\u001b[0m )\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Build the updated Q-values for the sampled future states\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Use the target model for stability\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m future_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_next_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Q value = reward + discount factor * expected future reward\u001b[39;00m\n\u001b[0;32m     69\u001b[0m updated_q_values \u001b[38;5;241m=\u001b[39m rewards_sample \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m keras\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mamax(\n\u001b[0;32m     70\u001b[0m     future_rewards, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     71\u001b[0m )\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:497\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    494\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    495\u001b[0m ):\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:715\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[1;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    714\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[1;32m--> 715\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[0;32m    717\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[0;32m    718\u001b[0m         dataset\n\u001b[0;32m    719\u001b[0m     )\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:231\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mwith_options(options)\n\u001b[0;32m    229\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n\u001b[1;32m--> 231\u001b[0m indices_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mindices_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslice_batch_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    233\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2389\u001b[0m, in \u001b[0;36mDatasetV2.flat_map\u001b[1;34m(self, map_func, name)\u001b[0m\n\u001b[0;32m   2385\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001b[39;00m\n\u001b[0;32m   2386\u001b[0m \u001b[38;5;66;03m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2387\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2388\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m flat_map_op\n\u001b[1;32m-> 2389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mflat_map_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:24\u001b[0m, in \u001b[0;36m_flat_map\u001b[1;34m(input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_flat_map\u001b[39m(input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_FlatMapDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:33\u001b[0m, in \u001b[0;36m_FlatMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, name)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dataset, map_func, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     32\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_dataset \u001b[38;5;241m=\u001b[39m input_dataset\n\u001b[1;32m---> 33\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func \u001b[38;5;241m=\u001b[39m \u001b[43mstructured_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStructuredFunctionWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmap_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transformation_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure, dataset_ops\u001b[38;5;241m.\u001b[39mDatasetSpec):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     37\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_ops\u001b[38;5;241m.\u001b[39mget_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_map_func\u001b[38;5;241m.\u001b[39moutput_structure)\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    258\u001b[0m       warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    259\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    260\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    261\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    262\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    263\u001b[0m     fn_factory \u001b[38;5;241m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function \u001b[38;5;241m=\u001b[39m \u001b[43mfn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    267\u001b[0m add_to_graph \u001b[38;5;241m&\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1251\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_concrete_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1250\u001b[0m   \u001b[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1251\u001b[0m   concrete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_concrete_function_garbage_collected\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m   concrete\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1221\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1219\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1220\u001b[0m     initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m   1225\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1226\u001b[0m   \u001b[38;5;66;03m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:303\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    299\u001b[0m placeholder_context \u001b[38;5;241m=\u001b[39m trace_type\u001b[38;5;241m.\u001b[39mInternalPlaceholderContext(\n\u001b[0;32m    300\u001b[0m     func_graph, type_context\u001b[38;5;241m.\u001b[39mget_placeholder_mapping()\n\u001b[0;32m    301\u001b[0m )\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m func_graph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m--> 303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m \u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_arguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:356\u001b[0m, in \u001b[0;36mFunctionType.placeholder_arguments\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not generate placeholder value for \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartially defined function type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    355\u001b[0m   placeholder_context\u001b[38;5;241m.\u001b[39mupdate_naming_scope(parameter\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m--> 356\u001b[0m   arguments[parameter\u001b[38;5;241m.\u001b[39mname] \u001b[38;5;241m=\u001b[39m \u001b[43mparameter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype_constraint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaceholder_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m      \u001b[49m\u001b[43mplaceholder_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39mBoundArguments(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:1021\u001b[0m, in \u001b[0;36mTensorSpec.placeholder_value\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_placeholder(context_graph, name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1021\u001b[0m   placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph_placeholder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontext_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1024\u001b[0m   \u001b[38;5;66;03m# Record the requested/user-specified name in case it's different than\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m   \u001b[38;5;66;03m# the uniquified name, for validation when exporting signatures.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m   placeholder\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39m_set_attr(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_user_specified_name\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1028\u001b[0m       attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(s\u001b[38;5;241m=\u001b[39mcompat\u001b[38;5;241m.\u001b[39mas_bytes(name)))\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:1059\u001b[0m, in \u001b[0;36mTensorSpec._graph_placeholder\u001b[1;34m(self, graph, name)\u001b[0m\n\u001b[0;32m   1057\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m: shape}\n\u001b[0;32m   1058\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1059\u001b[0m   op \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m   1060\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPlaceholder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1063\u001b[0m   \u001b[38;5;66;03m# TODO(b/262413656) Sometimes parameter names are not valid op names, in\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m   \u001b[38;5;66;03m# which case an unnamed placeholder is created instead. Update this logic\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m   \u001b[38;5;66;03m# to sanitize the name instead of falling back on unnamed placeholders.\u001b[39;00m\n\u001b[0;32m   1066\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(e)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_op_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m \u001b[43m    \u001b[49m\u001b[43mop_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_device\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2701\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2698\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2699\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2700\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2701\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mOperation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_node_def\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2702\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2703\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2704\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2705\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2706\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcontrol_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontrol_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2707\u001b[0m \u001b[43m      \u001b[49m\u001b[43minput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2708\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_op\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_default_original_op\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2709\u001b[0m \u001b[43m      \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2710\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2711\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1196\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1196\u001b[0m c_op \u001b[38;5;241m=\u001b[39m \u001b[43m_create_c_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode_def\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontrol_input_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_def\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mop_def\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1053\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1049\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(name),\n\u001b[0;32m   1050\u001b[0m                                          serialized)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1053\u001b[0m   c_op \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_FinishOperation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_desc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mInvalidArgumentError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1055\u001b[0m   \u001b[38;5;66;03m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().timestamp()\n",
    "\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    # print(observation.shape)\n",
    "\n",
    "    # break\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        # state_processed = preprocess_observation(state)\n",
    "        # state_next_processed = preprocess_observation(state_next)\n",
    "        state_processed = state\n",
    "        state_next_processed = state_next\n",
    "        # print(state_tensor.shape)\n",
    "\n",
    "        # Append data to replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state_processed)\n",
    "        state_next_history.append(state_next_processed)\n",
    "        done_history.append(float(done))\n",
    "        rewards_history.append(float(reward))\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(\n",
    "                range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([np.array(state_history[i]) for i in indices])\n",
    "            state_next_sample = np.array(\n",
    "                [np.array(state_next_history[i]) for i in indices])\n",
    "            rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
    "            action_sample = np.array([action_history[i] for i in indices], dtype=np.int32)\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices], dtype=tf.float32\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample, verbose=0)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * \\\n",
    "                (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(\n",
    "                    keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            log_file = f\"./logs/modelstats_{datetime.now():%d-%m}.csv\"\n",
    "            print(f\"best score of last 100: {np.max(episode_reward_history)}, running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}, time: {datetime.now().timestamp()-start_time}\")\n",
    "            with open(log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if os.stat(log_file).st_size == 0:\n",
    "                    writer.writerow([\"episode\", \"frame\", \"running_reward\", \"max_reward\", \"time\"])\n",
    "                writer.writerow([episode_count, frame_count, running_reward, np.max(episode_reward_history), (datetime.now().timestamp()-start_time)])\n",
    "                f.flush()\n",
    "            model.save(f\"./models/space_invaders_qmodel_{episode_count}.keras\")\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 500:  # 990 = max score for round\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (\n",
    "        max_episodes > 0 and episode_count >= max_episodes\n",
    "    ):  # Maximum number of episodes reached\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    if (max_frames <= frame_count):\n",
    "        print(f\"Stopped at frame {frame_count}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
