{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on Lec5-RL-Gymnasium.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers.frame_stack import FrameStack\n",
    "from gymnasium.wrappers.atari_preprocessing import AtariPreprocessing\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import ale_py\n",
    "from collections import deque\n",
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "gym.register_envs(ale_py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\.venv\\Lib\\site-packages\\gymnasium\\wrappers\\record_video.py:94: UserWarning: \u001b[33mWARN: Overwriting existing videos at c:\\KODA\\ITHS\\9_Deep Learning\\DeepLearningLaboration\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.1\n",
    "epsilon_max = 1.0\n",
    "epsilon_interval = (epsilon_max - epsilon_min)\n",
    "batch_size = 32\n",
    "max_steps_per_episode = 10000\n",
    "max_episodes = 0\n",
    "max_frames = 1e7\n",
    "\n",
    "env = gym.make(\"SpaceInvadersNoFrameskip-v4\", render_mode=\"rgb_array\")\n",
    "\n",
    "env = AtariPreprocessing(env)\n",
    "\n",
    "env = FrameStack(env, 4)\n",
    "def trigger(t):\n",
    "    return t % 100 == 0\n",
    "env = gym.wrappers.RecordVideo(env, video_folder=\"./videos\", episode_trigger=trigger, disable_logger=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_observation(observation):\n",
    "    observation = tf.transpose(observation, perm=[0, 1, 2])\n",
    "    observation = tf.cast(observation, tf.float32) / 255.0\n",
    "    return observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height, width, channels = env.observation_space.shape\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 6\n",
    "\n",
    "def create_q_model():\n",
    "    return keras.Sequential(\n",
    "        [\n",
    "            # layers.InputLayer(shape=(3, height, width, channels)),\n",
    "            layers.Permute((2, 3, 1)),  # Rearrange dimensions\n",
    "            layers.Conv2D(32, kernel_size=8, strides=4, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=4, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(64, kernel_size=3, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation=\"relu\"),\n",
    "            layers.Dense(256, activation=\"relu\"),\n",
    "            layers.Dense(num_actions, activation=\"linear\")\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "model = create_q_model()\n",
    "model_target = create_q_model()\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
    "\n",
    "# observation, _ = env.reset(seed=42)\n",
    "# state = np.array(observation)\n",
    "# state_tensor = keras.ops.convert_to_tensor(state)\n",
    "# state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "# print(state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ permute_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ permute_6 (\u001b[38;5;33mPermute\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_24 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_25 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_26 (\u001b[38;5;33mConv2D\u001b[0m)              │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_8 (\u001b[38;5;33mFlatten\u001b[0m)             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "done_history = []\n",
    "episode_reward_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "\n",
    "max_buffer_size = 100000\n",
    "action_history = deque(maxlen=max_buffer_size)\n",
    "state_history = deque(maxlen=max_buffer_size)\n",
    "state_next_history = deque(maxlen=max_buffer_size)\n",
    "done_history = deque(maxlen=max_buffer_size)\n",
    "rewards_history = deque(maxlen=max_buffer_size)\n",
    "\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 50000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 1000000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 1000000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 6\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "loss_function = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score of last 100: 385.0, running reward: 151.25 at episode 20, frame count 10000, time: 222.19386291503906\n",
      "best score of last 100: 410.0, running reward: 147.68 at episode 41, frame count 20000, time: 445.94836807250977\n",
      "best score of last 100: 500.0, running reward: 160.88 at episode 57, frame count 30000, time: 670.8054659366608\n",
      "best score of last 100: 535.0, running reward: 155.72 at episode 76, frame count 40000, time: 896.20130610466\n",
      "best score of last 100: 610.0, running reward: 160.63 at episode 95, frame count 50000, time: 1123.9240419864655\n",
      "best score of last 100: 610.0, running reward: 159.40 at episode 114, frame count 60000, time: 1358.9138169288635\n",
      "best score of last 100: 610.0, running reward: 162.10 at episode 133, frame count 70000, time: 1595.4292950630188\n",
      "best score of last 100: 610.0, running reward: 151.60 at episode 154, frame count 80000, time: 1832.0393500328064\n",
      "best score of last 100: 610.0, running reward: 149.55 at episode 174, frame count 90000, time: 2070.8949539661407\n",
      "best score of last 100: 395.0, running reward: 148.00 at episode 192, frame count 100000, time: 2311.6988401412964\n",
      "best score of last 100: 395.0, running reward: 154.10 at episode 211, frame count 110000, time: 2555.082041978836\n",
      "best score of last 100: 410.0, running reward: 145.40 at episode 232, frame count 120000, time: 2798.904606103897\n",
      "best score of last 100: 410.0, running reward: 151.20 at episode 252, frame count 130000, time: 3044.205421924591\n",
      "best score of last 100: 410.0, running reward: 146.55 at episode 273, frame count 140000, time: 3293.515326023102\n",
      "best score of last 100: 410.0, running reward: 139.60 at episode 293, frame count 150000, time: 3540.6685061454773\n",
      "best score of last 100: 440.0, running reward: 138.55 at episode 313, frame count 160000, time: 3788.5640881061554\n",
      "best score of last 100: 440.0, running reward: 144.60 at episode 332, frame count 170000, time: 4043.4826629161835\n",
      "best score of last 100: 490.0, running reward: 147.60 at episode 352, frame count 180000, time: 4303.467427968979\n",
      "best score of last 100: 490.0, running reward: 153.20 at episode 372, frame count 190000, time: 4566.510314941406\n",
      "best score of last 100: 515.0, running reward: 158.55 at episode 391, frame count 200000, time: 4829.8981590271\n",
      "best score of last 100: 515.0, running reward: 163.65 at episode 412, frame count 210000, time: 5084.528058052063\n",
      "best score of last 100: 515.0, running reward: 165.80 at episode 432, frame count 220000, time: 5342.162029027939\n",
      "best score of last 100: 515.0, running reward: 161.00 at episode 453, frame count 230000, time: 5606.803276062012\n",
      "best score of last 100: 515.0, running reward: 165.35 at episode 471, frame count 240000, time: 5866.733612060547\n",
      "best score of last 100: 515.0, running reward: 168.40 at episode 489, frame count 250000, time: 6136.545865058899\n",
      "best score of last 100: 515.0, running reward: 172.05 at episode 508, frame count 260000, time: 6398.730111122131\n",
      "best score of last 100: 515.0, running reward: 168.65 at episode 528, frame count 270000, time: 6660.291975975037\n",
      "best score of last 100: 515.0, running reward: 172.55 at episode 547, frame count 280000, time: 6934.498518943787\n",
      "best score of last 100: 515.0, running reward: 170.70 at episode 566, frame count 290000, time: 7201.102324008942\n",
      "best score of last 100: 610.0, running reward: 174.40 at episode 584, frame count 300000, time: 7463.286687135696\n",
      "best score of last 100: 610.0, running reward: 165.35 at episode 604, frame count 310000, time: 7724.448557138443\n",
      "best score of last 100: 610.0, running reward: 167.75 at episode 623, frame count 320000, time: 7991.38206911087\n",
      "best score of last 100: 610.0, running reward: 167.30 at episode 645, frame count 330000, time: 8257.325006961823\n",
      "best score of last 100: 610.0, running reward: 159.10 at episode 667, frame count 340000, time: 8520.046010017395\n",
      "best score of last 100: 550.0, running reward: 155.80 at episode 687, frame count 350000, time: 8783.048645973206\n",
      "best score of last 100: 550.0, running reward: 161.25 at episode 704, frame count 360000, time: 9046.4597260952\n",
      "best score of last 100: 590.0, running reward: 159.00 at episode 724, frame count 370000, time: 9310.978447914124\n",
      "best score of last 100: 590.0, running reward: 166.40 at episode 745, frame count 380000, time: 9575.67848610878\n",
      "best score of last 100: 715.0, running reward: 177.60 at episode 765, frame count 390000, time: 9841.140929937363\n",
      "best score of last 100: 715.0, running reward: 177.45 at episode 785, frame count 400000, time: 10107.275156021118\n",
      "best score of last 100: 775.0, running reward: 185.55 at episode 802, frame count 410000, time: 10374.671869039536\n",
      "best score of last 100: 775.0, running reward: 180.10 at episode 824, frame count 420000, time: 10642.543339967728\n",
      "best score of last 100: 775.0, running reward: 178.95 at episode 843, frame count 430000, time: 10910.612998008728\n",
      "best score of last 100: 775.0, running reward: 173.65 at episode 862, frame count 440000, time: 11179.420185089111\n",
      "best score of last 100: 775.0, running reward: 184.95 at episode 881, frame count 450000, time: 11448.887021064758\n",
      "best score of last 100: 515.0, running reward: 171.55 at episode 899, frame count 460000, time: 11718.775459051132\n",
      "best score of last 100: 515.0, running reward: 175.45 at episode 918, frame count 470000, time: 11993.558056116104\n",
      "best score of last 100: 565.0, running reward: 183.05 at episode 936, frame count 480000, time: 12269.443186044693\n",
      "best score of last 100: 675.0, running reward: 189.45 at episode 953, frame count 490000, time: 12544.60713005066\n",
      "best score of last 100: 675.0, running reward: 189.45 at episode 972, frame count 500000, time: 12820.11620092392\n",
      "best score of last 100: 675.0, running reward: 186.70 at episode 992, frame count 510000, time: 13096.171991109848\n",
      "best score of last 100: 675.0, running reward: 179.35 at episode 1011, frame count 520000, time: 13372.79499411583\n",
      "best score of last 100: 675.0, running reward: 179.70 at episode 1028, frame count 530000, time: 13649.792558908463\n",
      "best score of last 100: 675.0, running reward: 183.85 at episode 1046, frame count 540000, time: 13927.23755812645\n",
      "best score of last 100: 560.0, running reward: 182.50 at episode 1064, frame count 550000, time: 14204.590731143951\n",
      "best score of last 100: 560.0, running reward: 191.85 at episode 1082, frame count 560000, time: 14484.034143924713\n",
      "best score of last 100: 560.0, running reward: 199.85 at episode 1098, frame count 570000, time: 14763.874184131622\n",
      "best score of last 100: 560.0, running reward: 210.50 at episode 1117, frame count 580000, time: 15045.840069055557\n",
      "best score of last 100: 560.0, running reward: 210.00 at episode 1133, frame count 590000, time: 15327.377444028854\n",
      "best score of last 100: 655.0, running reward: 208.20 at episode 1153, frame count 600000, time: 15609.40456700325\n",
      "best score of last 100: 655.0, running reward: 220.05 at episode 1170, frame count 610000, time: 15892.467855930328\n",
      "best score of last 100: 655.0, running reward: 223.70 at episode 1189, frame count 620000, time: 16176.963968992233\n",
      "best score of last 100: 655.0, running reward: 222.85 at episode 1207, frame count 630000, time: 16461.460232019424\n",
      "best score of last 100: 655.0, running reward: 223.00 at episode 1225, frame count 640000, time: 16745.10237312317\n",
      "best score of last 100: 630.0, running reward: 217.45 at episode 1243, frame count 650000, time: 17029.736407995224\n",
      "best score of last 100: 630.0, running reward: 215.05 at episode 1263, frame count 660000, time: 17315.916906118393\n",
      "best score of last 100: 695.0, running reward: 211.90 at episode 1281, frame count 670000, time: 17602.314589977264\n",
      "best score of last 100: 695.0, running reward: 215.10 at episode 1298, frame count 680000, time: 17888.796617031097\n",
      "best score of last 100: 695.0, running reward: 214.20 at episode 1316, frame count 690000, time: 18177.815881967545\n",
      "best score of last 100: 695.0, running reward: 204.15 at episode 1335, frame count 700000, time: 18466.115972042084\n",
      "best score of last 100: 695.0, running reward: 205.10 at episode 1354, frame count 710000, time: 18756.43374800682\n",
      "best score of last 100: 870.0, running reward: 211.20 at episode 1371, frame count 720000, time: 19046.983822107315\n",
      "best score of last 100: 870.0, running reward: 205.35 at episode 1389, frame count 730000, time: 19338.265941143036\n",
      "best score of last 100: 870.0, running reward: 203.05 at episode 1407, frame count 740000, time: 19629.93686914444\n",
      "best score of last 100: 870.0, running reward: 216.15 at episode 1424, frame count 750000, time: 19922.406461000443\n",
      "best score of last 100: 870.0, running reward: 222.85 at episode 1440, frame count 760000, time: 20215.181450128555\n",
      "best score of last 100: 870.0, running reward: 234.25 at episode 1455, frame count 770000, time: 20509.676547050476\n",
      "best score of last 100: 820.0, running reward: 237.90 at episode 1471, frame count 780000, time: 20804.08711194992\n",
      "best score of last 100: 820.0, running reward: 243.60 at episode 1488, frame count 790000, time: 21099.384614944458\n",
      "best score of last 100: 820.0, running reward: 254.75 at episode 1504, frame count 800000, time: 21395.541391134262\n",
      "best score of last 100: 820.0, running reward: 258.75 at episode 1520, frame count 810000, time: 21692.33539891243\n",
      "best score of last 100: 820.0, running reward: 257.55 at episode 1535, frame count 820000, time: 21989.78669309616\n",
      "best score of last 100: 820.0, running reward: 252.05 at episode 1552, frame count 830000, time: 22288.005151987076\n",
      "best score of last 100: 820.0, running reward: 257.00 at episode 1568, frame count 840000, time: 22587.373548030853\n",
      "best score of last 100: 745.0, running reward: 252.30 at episode 1583, frame count 850000, time: 22887.142648935318\n",
      "best score of last 100: 745.0, running reward: 252.15 at episode 1599, frame count 860000, time: 23188.209571123123\n",
      "best score of last 100: 745.0, running reward: 251.05 at episode 1617, frame count 870000, time: 23490.782190084457\n",
      "best score of last 100: 745.0, running reward: 253.85 at episode 1631, frame count 880000, time: 23794.204049110413\n",
      "best score of last 100: 745.0, running reward: 253.90 at episode 1648, frame count 890000, time: 24097.76019191742\n",
      "best score of last 100: 695.0, running reward: 259.75 at episode 1662, frame count 900000, time: 24401.98137497902\n",
      "best score of last 100: 695.0, running reward: 255.80 at episode 1679, frame count 910000, time: 24706.67928004265\n",
      "best score of last 100: 695.0, running reward: 249.90 at episode 1695, frame count 920000, time: 25011.89382791519\n",
      "best score of last 100: 695.0, running reward: 250.40 at episode 1710, frame count 930000, time: 25320.617177963257\n",
      "best score of last 100: 665.0, running reward: 245.20 at episode 1726, frame count 940000, time: 25629.77445411682\n",
      "best score of last 100: 665.0, running reward: 248.95 at episode 1741, frame count 950000, time: 25939.259938001633\n",
      "best score of last 100: 665.0, running reward: 249.15 at episode 1755, frame count 960000, time: 26250.71068406105\n",
      "best score of last 100: 665.0, running reward: 252.95 at episode 1770, frame count 970000, time: 26561.684113025665\n",
      "best score of last 100: 730.0, running reward: 263.55 at episode 1785, frame count 980000, time: 26873.34936594963\n",
      "best score of last 100: 730.0, running reward: 269.60 at episode 1800, frame count 990000, time: 27186.3856010437\n",
      "best score of last 100: 730.0, running reward: 273.85 at episode 1816, frame count 1000000, time: 27500.073382139206\n",
      "best score of last 100: 730.0, running reward: 268.35 at episode 1830, frame count 1010000, time: 27812.700869083405\n",
      "best score of last 100: 730.0, running reward: 271.35 at episode 1845, frame count 1020000, time: 28125.663953065872\n",
      "best score of last 100: 770.0, running reward: 274.45 at episode 1860, frame count 1030000, time: 28438.652184963226\n",
      "best score of last 100: 770.0, running reward: 263.05 at episode 1873, frame count 1040000, time: 28750.734561920166\n",
      "best score of last 100: 770.0, running reward: 269.80 at episode 1888, frame count 1050000, time: 29063.563760995865\n",
      "best score of last 100: 770.0, running reward: 281.00 at episode 1900, frame count 1060000, time: 29376.196810007095\n",
      "best score of last 100: 835.0, running reward: 286.90 at episode 1915, frame count 1070000, time: 29689.58879494667\n",
      "best score of last 100: 835.0, running reward: 286.80 at episode 1930, frame count 1080000, time: 30002.889678955078\n",
      "best score of last 100: 835.0, running reward: 279.85 at episode 1946, frame count 1090000, time: 30316.697379112244\n",
      "best score of last 100: 835.0, running reward: 286.70 at episode 1959, frame count 1100000, time: 30629.738594055176\n",
      "best score of last 100: 835.0, running reward: 279.85 at episode 1976, frame count 1110000, time: 30942.858809947968\n",
      "best score of last 100: 835.0, running reward: 275.25 at episode 1990, frame count 1120000, time: 31255.733444929123\n",
      "best score of last 100: 835.0, running reward: 269.40 at episode 2006, frame count 1130000, time: 31569.24370598793\n",
      "best score of last 100: 725.0, running reward: 258.90 at episode 2021, frame count 1140000, time: 31882.492499113083\n",
      "best score of last 100: 725.0, running reward: 259.75 at episode 2037, frame count 1150000, time: 32195.514194965363\n",
      "best score of last 100: 725.0, running reward: 260.95 at episode 2053, frame count 1160000, time: 32508.947508096695\n",
      "best score of last 100: 725.0, running reward: 253.95 at episode 2067, frame count 1170000, time: 32821.79643702507\n",
      "best score of last 100: 725.0, running reward: 273.75 at episode 2079, frame count 1180000, time: 33133.98056292534\n",
      "best score of last 100: 705.0, running reward: 283.15 at episode 2092, frame count 1190000, time: 33447.80855512619\n",
      "best score of last 100: 705.0, running reward: 289.45 at episode 2107, frame count 1200000, time: 33761.48451113701\n",
      "best score of last 100: 705.0, running reward: 296.45 at episode 2121, frame count 1210000, time: 34075.2506480217\n",
      "best score of last 100: 705.0, running reward: 300.05 at episode 2136, frame count 1220000, time: 34389.43307709694\n",
      "best score of last 100: 705.0, running reward: 304.35 at episode 2149, frame count 1230000, time: 34702.94701504707\n",
      "best score of last 100: 705.0, running reward: 302.60 at episode 2164, frame count 1240000, time: 35016.8887629509\n",
      "best score of last 100: 655.0, running reward: 293.75 at episode 2178, frame count 1250000, time: 35331.019015073776\n",
      "best score of last 100: 655.0, running reward: 289.40 at episode 2194, frame count 1260000, time: 35645.24248504639\n",
      "best score of last 100: 620.0, running reward: 281.70 at episode 2208, frame count 1270000, time: 35959.45404291153\n",
      "best score of last 100: 730.0, running reward: 286.30 at episode 2223, frame count 1280000, time: 36273.65167403221\n",
      "best score of last 100: 750.0, running reward: 292.85 at episode 2235, frame count 1290000, time: 36587.13712000847\n",
      "best score of last 100: 750.0, running reward: 291.80 at episode 2250, frame count 1300000, time: 36901.512665987015\n",
      "best score of last 100: 750.0, running reward: 284.00 at episode 2266, frame count 1310000, time: 37216.12329697609\n",
      "best score of last 100: 750.0, running reward: 285.90 at episode 2280, frame count 1320000, time: 37530.99983596802\n",
      "best score of last 100: 750.0, running reward: 292.55 at episode 2294, frame count 1330000, time: 37845.89750909805\n",
      "best score of last 100: 750.0, running reward: 294.85 at episode 2308, frame count 1340000, time: 38160.4417951107\n",
      "best score of last 100: 750.0, running reward: 287.95 at episode 2322, frame count 1350000, time: 38475.554814100266\n",
      "best score of last 100: 745.0, running reward: 287.45 at episode 2336, frame count 1360000, time: 38789.96611595154\n",
      "best score of last 100: 745.0, running reward: 281.55 at episode 2352, frame count 1370000, time: 39104.97965312004\n",
      "best score of last 100: 745.0, running reward: 293.55 at episode 2367, frame count 1380000, time: 39419.85176610947\n",
      "best score of last 100: 745.0, running reward: 280.95 at episode 2382, frame count 1390000, time: 39735.168643951416\n",
      "best score of last 100: 745.0, running reward: 278.90 at episode 2397, frame count 1400000, time: 40047.97528004646\n",
      "best score of last 100: 745.0, running reward: 282.30 at episode 2411, frame count 1410000, time: 40360.57654809952\n",
      "best score of last 100: 745.0, running reward: 274.10 at episode 2426, frame count 1420000, time: 40672.678873062134\n",
      "best score of last 100: 600.0, running reward: 261.40 at episode 2443, frame count 1430000, time: 40984.75919294357\n",
      "best score of last 100: 590.0, running reward: 263.40 at episode 2458, frame count 1440000, time: 41296.983493089676\n",
      "best score of last 100: 590.0, running reward: 256.70 at episode 2475, frame count 1450000, time: 41610.86952710152\n",
      "best score of last 100: 590.0, running reward: 252.35 at episode 2491, frame count 1460000, time: 41931.47138595581\n",
      "best score of last 100: 590.0, running reward: 253.20 at episode 2508, frame count 1470000, time: 42252.01701903343\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now().timestamp()\n",
    "\n",
    "while True:\n",
    "    observation, _ = env.reset()\n",
    "    state = np.array(observation)\n",
    "    episode_reward = 0\n",
    "    # print(observation.shape)\n",
    "\n",
    "    # break\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_tensor = keras.ops.convert_to_tensor(state)\n",
    "            state_tensor = keras.ops.expand_dims(state_tensor, 0)\n",
    "            action_probs = model(state_tensor, training=False)\n",
    "            # Take best action\n",
    "            action = keras.ops.argmax(action_probs[0]).numpy()\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        # state_processed = preprocess_observation(state)\n",
    "        # state_next_processed = preprocess_observation(state_next)\n",
    "        state_processed = state\n",
    "        state_next_processed = state_next\n",
    "        # print(state_tensor.shape)\n",
    "\n",
    "        # Append data to replay buffer\n",
    "        action_history.append(action)\n",
    "        state_history.append(state_processed)\n",
    "        state_next_history.append(state_next_processed)\n",
    "        done_history.append(float(done))\n",
    "        rewards_history.append(float(reward))\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(\n",
    "                range(len(done_history)), size=batch_size)\n",
    "\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([np.array(state_history[i]) for i in indices])\n",
    "            state_next_sample = np.array(\n",
    "                [np.array(state_next_history[i]) for i in indices])\n",
    "            rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n",
    "            action_sample = np.array([action_history[i] for i in indices], dtype=np.int32)\n",
    "            done_sample = keras.ops.convert_to_tensor(\n",
    "                [float(done_history[i]) for i in indices], dtype=tf.float32\n",
    "            )\n",
    "\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample, verbose=0)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * keras.ops.amax(\n",
    "                future_rewards, axis=1\n",
    "            )\n",
    "\n",
    "            # If final frame set the last value to -1\n",
    "            updated_q_values = updated_q_values * \\\n",
    "                (1 - done_sample) - done_sample\n",
    "\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = keras.ops.one_hot(action_sample, num_actions)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Train the model on the states and updated Q-values\n",
    "                q_values = model(state_sample)\n",
    "\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = keras.ops.sum(\n",
    "                    keras.ops.multiply(q_values, masks), axis=1)\n",
    "                # Calculate loss between new Q-value and old Q-value\n",
    "                loss = loss_function(updated_q_values, q_action)\n",
    "\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            log_file = f\"./logs/modelstats_{datetime.now():%d-%m}.csv\"\n",
    "            print(f\"best score of last 100: {np.max(episode_reward_history)}, running reward: {running_reward:.2f} at episode {episode_count}, frame count {frame_count}, time: {datetime.now().timestamp()-start_time}\")\n",
    "            with open(log_file, \"a\", newline=\"\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                if os.stat(log_file).st_size == 0:\n",
    "                    writer.writerow([\"episode\", \"frame\", \"running_reward\", \"max_reward\", \"time\"])\n",
    "                writer.writerow([episode_count, frame_count, running_reward, np.max(episode_reward_history), (datetime.now().timestamp()-start_time)])\n",
    "                f.flush()\n",
    "            model.save(f\"./models/space_invaders_qmodel_{episode_count}.keras\")\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    if len(episode_reward_history) > 100:\n",
    "        del episode_reward_history[:1]\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "\n",
    "    episode_count += 1\n",
    "\n",
    "    if running_reward > 500:  # 990 = max score for round\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "\n",
    "    if (\n",
    "        max_episodes > 0 and episode_count >= max_episodes\n",
    "    ):  # Maximum number of episodes reached\n",
    "        print(\"Stopped at episode {}!\".format(episode_count))\n",
    "        break\n",
    "    if (max_frames <= frame_count):\n",
    "        print(f\"Stopped at frame {frame_count}!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
